{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e63a8f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\ActiveSemisupervisedLearningCNN-s\\tf_env\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.9273 - loss: 0.2430 - val_accuracy: 0.9796 - val_loss: 0.0709\n",
      "Epoch 2/5\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 34ms/step - accuracy: 0.9797 - loss: 0.0650 - val_accuracy: 0.9853 - val_loss: 0.0507\n",
      "Epoch 3/5\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 34ms/step - accuracy: 0.9862 - loss: 0.0447 - val_accuracy: 0.9868 - val_loss: 0.0467\n",
      "Epoch 4/5\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9893 - loss: 0.0348 - val_accuracy: 0.9880 - val_loss: 0.0422\n",
      "Epoch 5/5\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9913 - loss: 0.0278 - val_accuracy: 0.9873 - val_loss: 0.0435\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9877 - loss: 0.0358\n",
      "Loss: 0.0358, Accuracy: 0.9877\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "\n",
    "\n",
    "# Carregar os dados\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "\n",
    "# Pr√©-processamento\n",
    "\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "\n",
    "x_test  = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "\n",
    "y_test  = to_categorical(y_test, 10)\n",
    "\n",
    "\n",
    "\n",
    "# Definir a CNN\n",
    "\n",
    "model = Sequential([\n",
    "\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(128, activation='relu'),\n",
    "\n",
    "    Dense(10, activation='softmax')\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Compilar o modelo\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "\n",
    "              loss='categorical_crossentropy',\n",
    "\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Treinar o modelo\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "\n",
    "          epochs=5,\n",
    "\n",
    "          batch_size=128,\n",
    "\n",
    "          validation_split=0.2)\n",
    "\n",
    "\n",
    "\n",
    "# Avaliar no conjunto de teste\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f\"Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25863e7a",
   "metadata": {},
   "source": [
    "### Config (dimens√µes, batch, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "266bfc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, json, math, random, pathlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# ==== CONFIG ====\n",
    "H, W = 180, 180\n",
    "IMAGE_SIZE = (H, W)\n",
    "BATCH = 64\n",
    "NUM_CLASSES = 2\n",
    "CLASS_NAMES = (\"Cat\", \"Dog\")   # nomes como est√£o nas pastas\n",
    "CLASS_MAP = {name: i for i, name in enumerate(CLASS_NAMES)}\n",
    "INV_CLASS_MAP = {v:k for k,v in CLASS_MAP.items()}\n",
    "\n",
    "tau = 0.8          # threshold inicial mais brando\n",
    "lambda_u_max = 1.0 # peso m√°ximo da perda unsupervisionada\n",
    "c_u, c_i = 1.0, 1.0  # coef. UCB\n",
    "alpha = 0.9          # EMA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3da6b4",
   "metadata": {},
   "source": [
    "### Helpers de leitura e record builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6bc9065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\felip\\AppData\\Local\\Temp\\ipykernel_10544\\2949202176.py:9: ignore_errors (from tensorflow.python.data.experimental.ops.error_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.ignore_errors` instead.\n"
     ]
    }
   ],
   "source": [
    "def _safe_decode_resize_jpeg(path, image_size=IMAGE_SIZE):\n",
    "    raw = tf.io.read_file(path)\n",
    "    # O PetImages tem JPEGs (√†s vezes corrompidos). Se tiver PNG misturado use decode_image.\n",
    "    img = tf.image.decode_jpeg(raw, channels=3)\n",
    "    img = tf.image.resize(img, image_size)\n",
    "    img = tf.cast(img, tf.float32)  # 0..255 (normalizamos no modelo)\n",
    "    return img\n",
    "\n",
    "IGNORE_ERRORS = tf.data.experimental.ignore_errors()\n",
    "\n",
    "def build_records_from_dir(root_dir, class_map=CLASS_MAP, with_ids=True, seed=42):\n",
    "    \"\"\"\n",
    "    Varre .../Cat/*.jpg e .../Dog/*.jpg gerando records {sid, path, label}\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    records = []\n",
    "    sid = 0\n",
    "    for name, idx in class_map.items():\n",
    "        cls_dir = os.path.join(root_dir, name)\n",
    "        for fn in os.listdir(cls_dir):\n",
    "            if not fn.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                continue\n",
    "            p = os.path.join(cls_dir, fn)\n",
    "            rec = {\"sid\": sid, \"path\": p}\n",
    "            if class_map is not None:\n",
    "                rec[\"label\"] = idx\n",
    "            records.append(rec)\n",
    "            sid += 1\n",
    "    rng.shuffle(records)\n",
    "    return records\n",
    "\n",
    "def split_L_U(records, n_L=400, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    arr = records.copy()\n",
    "    rng.shuffle(arr)\n",
    "    L = []\n",
    "    U = []\n",
    "    for r in arr:\n",
    "        if len(L) < n_L:\n",
    "            L.append(r)\n",
    "        else:\n",
    "            U.append(r)\n",
    "    return L, U\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832d6a60",
   "metadata": {},
   "source": [
    "### Augmentations batch-first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "943cd0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_aug_seq = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomTranslation(0.05, 0.05),\n",
    "], name=\"weak_aug\")\n",
    "\n",
    "strong_aug_seq = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.15),\n",
    "    layers.RandomZoom(0.15),\n",
    "    layers.RandomContrast(0.15),\n",
    "], name=\"strong_aug\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ebf26d",
   "metadata": {},
   "source": [
    "### Datasets (vers√µes √∫nicas, sem duplicidade de batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e25a4f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_L_ds(L_records, batch=BATCH, image_size=IMAGE_SIZE, drop_remainder=False):\n",
    "    paths = [r[\"path\"] for r in L_records]\n",
    "    labels = [r[\"label\"] for r in L_records]\n",
    "    ids    = [r[\"sid\"]   for r in L_records]\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels, ids))\n",
    "    def _load(path, y, sid):\n",
    "        img = _safe_decode_resize_jpeg(path, image_size)\n",
    "        return img, y, sid\n",
    "    ds = ds.shuffle(4096, reshuffle_each_iteration=True)\n",
    "    ds = ds.map(_load, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    ds = ds.apply(IGNORE_ERRORS)\n",
    "    ds = ds.batch(batch, drop_remainder=drop_remainder)  # batch antes de aug\n",
    "    # Sem aug no L por enquanto (poderia adicionar augment leve aqui tamb√©m)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def make_U_ds(U_records, batch=BATCH, image_size=IMAGE_SIZE, drop_remainder=False):\n",
    "    paths = [r[\"path\"] for r in U_records]\n",
    "    ids   = [r[\"sid\"]  for r in U_records]\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, ids))\n",
    "    def _loadU(path, sid):\n",
    "        img = _safe_decode_resize_jpeg(path, image_size)\n",
    "        return img, sid\n",
    "    ds = ds.shuffle(4096, reshuffle_each_iteration=True)\n",
    "    ds = ds.map(_loadU, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    ds = ds.apply(IGNORE_ERRORS)\n",
    "    ds = ds.batch(batch, drop_remainder=drop_remainder)  # batch\n",
    "    def _make_pair(batch_img, batch_sid):\n",
    "        x_w = weak_aug_seq(batch_img, training=True)\n",
    "        x_s = strong_aug_seq(batch_img, training=True)\n",
    "        return x_w, x_s, batch_sid\n",
    "    ds = ds.map(_make_pair, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def make_val_ds(val_records, batch=BATCH, image_size=IMAGE_SIZE):\n",
    "    paths = [r[\"path\"] for r in val_records]\n",
    "    labels= [r[\"label\"] for r in val_records]\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    def _load(path, y):\n",
    "        img = _safe_decode_resize_jpeg(path, image_size)\n",
    "        return img, y\n",
    "    ds = ds.map(_load, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c180f",
   "metadata": {},
   "source": [
    "### Modelo √∫nico com normaliza√ß√£o no modelo (evita duplicidade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f9f825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes=NUM_CLASSES, image_size=IMAGE_SIZE):\n",
    "    inp = keras.Input(shape=image_size + (3,))\n",
    "    x = layers.Rescaling(1./255)(inp)            # normaliza TUDO (L,U,Val) no mesmo lugar\n",
    "    x = layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    out = layers.Dense(num_classes)(x)           # logits\n",
    "    model = keras.Model(inp, out)\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "optimizer = keras.optimizers.Adam(1e-3)\n",
    "loss_obj  = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d0811a",
   "metadata": {},
   "source": [
    "### M√©tricas por amostra, EMA/UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6535e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_logits(logits):\n",
    "    return tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "def pseudo_el2n(probs):  # incerteza (EL2N-like, proxy simples)\n",
    "    # dist√¢ncia do one-hot do argmax\n",
    "    yhat = tf.argmax(probs, axis=-1)\n",
    "    oh = tf.one_hot(yhat, depth=tf.shape(probs)[-1])\n",
    "    return tf.reduce_mean(tf.square(probs - oh), axis=-1)\n",
    "\n",
    "def sym_kl(p, q, eps=1e-7):\n",
    "    p = tf.clip_by_value(p, eps, 1.0)\n",
    "    q = tf.clip_by_value(q, eps, 1.0)\n",
    "    kl1 = tf.reduce_sum(p * tf.math.log(p/q), axis=-1)\n",
    "    kl2 = tf.reduce_sum(q * tf.math.log(q/p), axis=-1)\n",
    "    return 0.5*(kl1+kl2)\n",
    "\n",
    "# tabelas EMA\n",
    "u_mean, u_var = {}, {}\n",
    "i_mean, i_var = {}, {}\n",
    "\n",
    "def ema_update(mean_dict, var_dict, sid, val, alpha=0.9):\n",
    "    m = mean_dict.get(sid, val)\n",
    "    v = var_dict.get(sid, 0.0)\n",
    "    new_m = alpha*m + (1.0-alpha)*val\n",
    "    new_v = alpha*v + (1.0-alpha)*(val - new_m)**2\n",
    "    mean_dict[sid] = float(new_m)\n",
    "    var_dict[sid]  = float(new_v)\n",
    "\n",
    "def ucb(mu, var, c=1.0):\n",
    "    return mu + c * math.sqrt(max(var, 1e-12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7668c7",
   "metadata": {},
   "source": [
    "### Passos de treino (supervisionado e n√£o-rotulado) ‚Äî sem dupla normaliza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71c2f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_supervised(batch):\n",
    "    x_l, y_l, _ = batch\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits_l = model(x_l, training=True)  # Rescaling acontece no modelo\n",
    "        loss_sup = loss_obj(y_l, logits_l)\n",
    "    grads = tape.gradient(loss_sup, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits_l, -1), tf.cast(y_l, tf.int64)), tf.float32))\n",
    "    return loss_sup, acc\n",
    "\n",
    "def lambda_u_warmup(epoch, total=3, max_val=1.0):\n",
    "    return max_val * min(1.0, (epoch+1)/total)\n",
    "\n",
    "@tf.function\n",
    "def train_step_unlabeled(batch, lambda_u=1.0):\n",
    "    x_w, x_s, sids = batch\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits_w = model(x_w, training=True)\n",
    "        logits_s = model(x_s, training=True)\n",
    "        probs_w  = softmax_logits(logits_w)\n",
    "        probs_s  = softmax_logits(logits_s)\n",
    "\n",
    "        conf = tf.reduce_max(probs_w, axis=-1)\n",
    "        yhat = tf.argmax(probs_w, axis=-1)\n",
    "        mask = conf >= tau\n",
    "\n",
    "        loss_unsup = tf.constant(0., dtype=tf.float32)\n",
    "        masked = tf.where(mask)\n",
    "        if tf.shape(masked)[0] > 0:\n",
    "            logits_s_mask = tf.gather(logits_s, masked[:,0])\n",
    "            yhat_mask     = tf.gather(yhat, masked[:,0])\n",
    "            loss_unsup = loss_obj(yhat_mask, logits_s_mask)\n",
    "\n",
    "        loss = lambda_u * loss_unsup\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    u_batch = pseudo_el2n(probs_w)\n",
    "    i_batch = sym_kl(probs_w, probs_s)\n",
    "    return loss, u_batch, i_batch, sids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e9c674",
   "metadata": {},
   "source": [
    "### Avalia√ß√£o e execu√ß√£o de uma rodada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baa562cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_ds):\n",
    "    tot, ok, loss = 0, 0, 0.0\n",
    "    for x,y in val_ds:\n",
    "        logits = model(x, training=False)\n",
    "        loss += float(loss_obj(y, logits))\n",
    "        ok += int((tf.argmax(logits,-1).numpy() == y.numpy()).sum())\n",
    "        tot += y.shape[0]\n",
    "    return loss/max(len(list(val_ds)) or 1,1), ok/max(tot,1)\n",
    "\n",
    "def run_one_round(L_ds, U_ds, epoch_idx, epochs=5):\n",
    "    for ep in range(epochs):\n",
    "        # supervisionado\n",
    "        for batch in L_ds:\n",
    "            _ = train_step_supervised(batch)\n",
    "        # n√£o-rotulado\n",
    "        lam = lambda_u_warmup(ep, total=3, max_val=lambda_u_max)\n",
    "        for batch in U_ds:\n",
    "            loss_unsup, u_batch, i_batch, sids = train_step_unlabeled(batch, lambda_u=lam)\n",
    "\n",
    "            # atualiza EMA/UCB (use .numpy().item() p/ evitar erro de scalar)\n",
    "            u_vals = u_batch.numpy()\n",
    "            i_vals = i_batch.numpy()\n",
    "            sid_vals = sids.numpy()\n",
    "            for val_u, val_i, sid in zip(u_vals, i_vals, sid_vals):\n",
    "                ema_update(u_mean, u_var, int(sid), float(np.asarray(val_u).item()), alpha)\n",
    "                ema_update(i_mean, i_var, int(sid), float(np.asarray(val_i).item()), alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82b14a2",
   "metadata": {},
   "source": [
    "### Sele√ß√£o, simula√ß√£o de anota√ß√£o e loop ASSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb5e0e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Adicionado ao PYTHONPATH: C:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\ActiveSemisupervisedLearningCNN-s\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve() # Sobe um n√≠vel e resolve o caminho absoluto\n",
    "\n",
    "# 2. Adiciona o caminho da raiz ao sys.path\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "    print(f\"‚úÖ Adicionado ao PYTHONPATH: {PROJECT_ROOT}\")\n",
    "else:\n",
    "    print(f\"üìÅ Pasta raiz j√° est√° no PYTHONPATH: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff35b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Rodada 1/10 ==\n"
     ]
    }
   ],
   "source": [
    "def acquire_topK_balanced(U_records, K, class_names=CLASS_NAMES):\n",
    "    per_class = max(1, K // max(1, len(class_names)))\n",
    "    buckets = {name: [] for name in class_names}\n",
    "\n",
    "    for rec in U_records:\n",
    "        sid = rec[\"sid\"]\n",
    "        p = rec[\"path\"].replace(\"\\\\\",\"/\").lower()\n",
    "        cname = None\n",
    "        for name in class_names:\n",
    "            if f\"/{name.lower()}/\" in p:\n",
    "                cname = name; break\n",
    "        if cname is None: \n",
    "            continue\n",
    "        mu_u, vu_u = u_mean.get(sid, 0.0), u_var.get(sid, 0.0)\n",
    "        mu_i, vi_i = i_mean.get(sid, 0.0), i_var.get(sid, 0.0)\n",
    "        score = ucb(mu_u, vu_u, c_u) * ucb(mu_i, vi_i, c_i)\n",
    "        buckets[cname].append((score, sid))\n",
    "\n",
    "    selected = []\n",
    "    for cname in class_names:\n",
    "        cand = sorted(buckets[cname], key=lambda x: x[0], reverse=True)\n",
    "        selected.extend([sid for _, sid in cand[:per_class]])\n",
    "\n",
    "    if len(selected) < K:\n",
    "        rest = []\n",
    "        have = set(selected)\n",
    "        for rec in U_records:\n",
    "            sid = rec[\"sid\"]\n",
    "            if sid in have: continue\n",
    "            mu_u, vu_u = u_mean.get(sid, 0.0), u_var.get(sid, 0.0)\n",
    "            mu_i, vi_i = i_mean.get(sid, 0.0), i_var.get(sid, 0.0)\n",
    "            rest.append((ucb(mu_u,vu_u,c_u)*ucb(mu_i,vi_i,c_i), sid))\n",
    "        rest.sort(key=lambda x: x[0], reverse=True)\n",
    "        selected.extend([sid for _, sid in rest[:K-len(selected)]])\n",
    "    return selected[:K]\n",
    "\n",
    "def simulate_sid2true_from_path(U_records, L_records, class_map=CLASS_MAP):\n",
    "    sid2true = {}\n",
    "    def infer(path):\n",
    "        p = path.replace(\"\\\\\",\"/\").lower()\n",
    "        for name, idx in class_map.items():\n",
    "            if f\"/{name.lower()}/\" in p:\n",
    "                return idx\n",
    "        raise ValueError(f\"N√£o infere classe de: {path}\")\n",
    "    for rec in U_records: sid2true[rec[\"sid\"]] = infer(rec[\"path\"])\n",
    "    for rec in L_records: sid2true[rec[\"sid\"]] = infer(rec[\"path\"])\n",
    "    return sid2true\n",
    "\n",
    "def move_annotated_to_L(sid2label, U_records, L_records):\n",
    "    by_id = {r[\"sid\"]: r for r in U_records}\n",
    "    moved = []\n",
    "    keepU = []\n",
    "    for r in U_records:\n",
    "        sid = r[\"sid\"]\n",
    "        if sid in sid2label:\n",
    "            nr = dict(r)\n",
    "            nr[\"label\"] = int(sid2label[sid])\n",
    "            moved.append(nr)\n",
    "        else:\n",
    "            keepU.append(r)\n",
    "    return keepU, (L_records + moved)\n",
    "\n",
    "# ==== Dataset base ====\n",
    "train_root = r\"..\\data\\train\"\n",
    "val_root   = r\"..\\data\\validation\"\n",
    "\n",
    "all_train = build_records_from_dir(train_root, class_map=CLASS_MAP)\n",
    "L_records, U_records = split_L_U(all_train, n_L=400, seed=42)\n",
    "\n",
    "val_records = build_records_from_dir(val_root, class_map=CLASS_MAP)\n",
    "val_ds = make_val_ds(val_records, batch=BATCH, image_size=IMAGE_SIZE)\n",
    "\n",
    "L_ds = make_L_ds(L_records, batch=BATCH, image_size=IMAGE_SIZE)\n",
    "U_ds = make_U_ds(U_records, batch=BATCH, image_size=IMAGE_SIZE)\n",
    "\n",
    "# simula√ß√£o de anota√ß√£o (sem arquivos)\n",
    "sid2true = simulate_sid2true_from_path(U_records, L_records, class_map=CLASS_MAP)\n",
    "\n",
    "ROUNDS = 10\n",
    "EPOCHS_PER_ROUND = 5\n",
    "\n",
    "for r in range(ROUNDS):\n",
    "    print(f\"== Rodada {r+1}/{ROUNDS} ==\")\n",
    "    run_one_round(L_ds, U_ds, epoch_idx=r, epochs=EPOCHS_PER_ROUND)\n",
    "\n",
    "    # avalia√ß√£o\n",
    "    vloss, vacc = evaluate(model, val_ds)\n",
    "    print(f\"Val loss: {vloss:.4f}  Val acc: {vacc:.4f}\")\n",
    "\n",
    "    # sele√ß√£o ativa balanceada\n",
    "    selected_ids = acquire_topK_balanced(U_records, K=100, class_names=CLASS_NAMES)\n",
    "\n",
    "    # ‚Äúanota√ß√£o‚Äù simulada apenas nos K escolhidos\n",
    "    sid2label = {sid: sid2true[sid] for sid in selected_ids}\n",
    "    # mover U->L\n",
    "    U_records, L_records = move_annotated_to_L(sid2label, U_records, L_records)\n",
    "\n",
    "    # limpar EMA/UCB dos sids movidos\n",
    "    moved_set = set(sid2label.keys())\n",
    "    for sid in list(u_mean.keys()):\n",
    "        if sid in moved_set: u_mean.pop(sid, None); u_var.pop(sid, None)\n",
    "    for sid in list(i_mean.keys()):\n",
    "        if sid in moved_set: i_mean.pop(sid, None); i_var.pop(sid, None)\n",
    "\n",
    "    # rebuild datasets\n",
    "    L_ds = make_L_ds(L_records, batch=BATCH, image_size=IMAGE_SIZE)\n",
    "    U_ds = make_U_ds(U_records, batch=BATCH, image_size=IMAGE_SIZE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
