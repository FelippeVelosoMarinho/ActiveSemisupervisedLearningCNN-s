{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e63a8f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\ActiveSemisupervisedLearningCNN-s\\tf_env\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.9273 - loss: 0.2430 - val_accuracy: 0.9796 - val_loss: 0.0709\n",
      "Epoch 2/5\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 34ms/step - accuracy: 0.9797 - loss: 0.0650 - val_accuracy: 0.9853 - val_loss: 0.0507\n",
      "Epoch 3/5\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 34ms/step - accuracy: 0.9862 - loss: 0.0447 - val_accuracy: 0.9868 - val_loss: 0.0467\n",
      "Epoch 4/5\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9893 - loss: 0.0348 - val_accuracy: 0.9880 - val_loss: 0.0422\n",
      "Epoch 5/5\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9913 - loss: 0.0278 - val_accuracy: 0.9873 - val_loss: 0.0435\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9877 - loss: 0.0358\n",
      "Loss: 0.0358, Accuracy: 0.9877\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "\n",
    "\n",
    "# Carregar os dados\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "\n",
    "# Pr√©-processamento\n",
    "\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "\n",
    "x_test  = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "\n",
    "y_test  = to_categorical(y_test, 10)\n",
    "\n",
    "\n",
    "\n",
    "# Definir a CNN\n",
    "\n",
    "model = Sequential([\n",
    "\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(128, activation='relu'),\n",
    "\n",
    "    Dense(10, activation='softmax')\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Compilar o modelo\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "\n",
    "              loss='categorical_crossentropy',\n",
    "\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Treinar o modelo\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "\n",
    "          epochs=5,\n",
    "\n",
    "          batch_size=128,\n",
    "\n",
    "          validation_split=0.2)\n",
    "\n",
    "\n",
    "\n",
    "# Avaliar no conjunto de teste\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f\"Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25863e7a",
   "metadata": {},
   "source": [
    "### Config (dimens√µes, batch, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "266bfc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, json, math, random, pathlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# ==== CONFIG ====\n",
    "H, W = 180, 180\n",
    "IMAGE_SIZE = (H, W)\n",
    "BATCH = 64\n",
    "NUM_CLASSES = 2\n",
    "CLASS_NAMES = (\"Cat\", \"Dog\")   # nomes como est√£o nas pastas\n",
    "CLASS_MAP = {name: i for i, name in enumerate(CLASS_NAMES)}\n",
    "INV_CLASS_MAP = {v:k for k,v in CLASS_MAP.items()}\n",
    "\n",
    "tau = 0.8          # threshold inicial mais brando\n",
    "lambda_u_max = 1.0 # peso m√°ximo da perda unsupervisionada\n",
    "c_u, c_i = 1.0, 1.0  # coef. UCB\n",
    "alpha = 0.9          # EMA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3da6b4",
   "metadata": {},
   "source": [
    "### Helpers de leitura e record builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6bc9065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\felip\\AppData\\Local\\Temp\\ipykernel_10544\\2949202176.py:9: ignore_errors (from tensorflow.python.data.experimental.ops.error_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.ignore_errors` instead.\n"
     ]
    }
   ],
   "source": [
    "def _safe_decode_resize_jpeg(path, image_size=IMAGE_SIZE):\n",
    "    raw = tf.io.read_file(path)\n",
    "    # O PetImages tem JPEGs (√†s vezes corrompidos). Se tiver PNG misturado use decode_image.\n",
    "    img = tf.image.decode_jpeg(raw, channels=3)\n",
    "    img = tf.image.resize(img, image_size)\n",
    "    img = tf.cast(img, tf.float32)  # 0..255 (normalizamos no modelo)\n",
    "    return img\n",
    "\n",
    "IGNORE_ERRORS = tf.data.experimental.ignore_errors()\n",
    "\n",
    "def build_records_from_dir(root_dir, class_map=CLASS_MAP, with_ids=True, seed=42):\n",
    "    \"\"\"\n",
    "    Varre .../Cat/*.jpg e .../Dog/*.jpg gerando records {sid, path, label}\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    records = []\n",
    "    sid = 0\n",
    "    for name, idx in class_map.items():\n",
    "        cls_dir = os.path.join(root_dir, name)\n",
    "        for fn in os.listdir(cls_dir):\n",
    "            if not fn.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                continue\n",
    "            p = os.path.join(cls_dir, fn)\n",
    "            rec = {\"sid\": sid, \"path\": p}\n",
    "            if class_map is not None:\n",
    "                rec[\"label\"] = idx\n",
    "            records.append(rec)\n",
    "            sid += 1\n",
    "    rng.shuffle(records)\n",
    "    return records\n",
    "\n",
    "def split_L_U(records, n_L=400, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    arr = records.copy()\n",
    "    rng.shuffle(arr)\n",
    "    L = []\n",
    "    U = []\n",
    "    for r in arr:\n",
    "        if len(L) < n_L:\n",
    "            L.append(r)\n",
    "        else:\n",
    "            U.append(r)\n",
    "    return L, U\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832d6a60",
   "metadata": {},
   "source": [
    "### Augmentations batch-first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "943cd0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_aug_seq = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomTranslation(0.05, 0.05),\n",
    "], name=\"weak_aug\")\n",
    "\n",
    "strong_aug_seq = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.15),\n",
    "    layers.RandomZoom(0.15),\n",
    "    layers.RandomContrast(0.15),\n",
    "], name=\"strong_aug\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ebf26d",
   "metadata": {},
   "source": [
    "### Datasets (vers√µes √∫nicas, sem duplicidade de batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e25a4f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_L_ds(L_records, batch=BATCH, image_size=IMAGE_SIZE, drop_remainder=False):\n",
    "    paths = [r[\"path\"] for r in L_records]\n",
    "    labels = [r[\"label\"] for r in L_records]\n",
    "    ids    = [r[\"sid\"]   for r in L_records]\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels, ids))\n",
    "    def _load(path, y, sid):\n",
    "        img = _safe_decode_resize_jpeg(path, image_size)\n",
    "        return img, y, sid\n",
    "    ds = ds.shuffle(4096, reshuffle_each_iteration=True)\n",
    "    ds = ds.map(_load, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    ds = ds.apply(IGNORE_ERRORS)\n",
    "    ds = ds.batch(batch, drop_remainder=drop_remainder)  # batch antes de aug\n",
    "    # Sem aug no L por enquanto (poderia adicionar augment leve aqui tamb√©m)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def make_U_ds(U_records, batch=BATCH, image_size=IMAGE_SIZE, drop_remainder=False):\n",
    "    paths = [r[\"path\"] for r in U_records]\n",
    "    ids   = [r[\"sid\"]  for r in U_records]\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, ids))\n",
    "    def _loadU(path, sid):\n",
    "        img = _safe_decode_resize_jpeg(path, image_size)\n",
    "        return img, sid\n",
    "    ds = ds.shuffle(4096, reshuffle_each_iteration=True)\n",
    "    ds = ds.map(_loadU, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    ds = ds.apply(IGNORE_ERRORS)\n",
    "    ds = ds.batch(batch, drop_remainder=drop_remainder)  # batch\n",
    "    def _make_pair(batch_img, batch_sid):\n",
    "        x_w = weak_aug_seq(batch_img, training=True)\n",
    "        x_s = strong_aug_seq(batch_img, training=True)\n",
    "        return x_w, x_s, batch_sid\n",
    "    ds = ds.map(_make_pair, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def make_val_ds(val_records, batch=BATCH, image_size=IMAGE_SIZE):\n",
    "    paths = [r[\"path\"] for r in val_records]\n",
    "    labels= [r[\"label\"] for r in val_records]\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    def _load(path, y):\n",
    "        img = _safe_decode_resize_jpeg(path, image_size)\n",
    "        return img, y\n",
    "    ds = ds.map(_load, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c180f",
   "metadata": {},
   "source": [
    "### Modelo √∫nico com normaliza√ß√£o no modelo (evita duplicidade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f9f825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes=NUM_CLASSES, image_size=IMAGE_SIZE):\n",
    "    inp = keras.Input(shape=image_size + (3,))\n",
    "    x = layers.Rescaling(1./255)(inp)            # normaliza TUDO (L,U,Val) no mesmo lugar\n",
    "    x = layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    out = layers.Dense(num_classes)(x)           # logits\n",
    "    model = keras.Model(inp, out)\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "optimizer = keras.optimizers.Adam(1e-3)\n",
    "loss_obj  = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d0811a",
   "metadata": {},
   "source": [
    "### M√©tricas por amostra, EMA/UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6535e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_logits(logits):\n",
    "    return tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "def pseudo_el2n(probs):  # incerteza (EL2N-like, proxy simples)\n",
    "    # dist√¢ncia do one-hot do argmax\n",
    "    yhat = tf.argmax(probs, axis=-1)\n",
    "    oh = tf.one_hot(yhat, depth=tf.shape(probs)[-1])\n",
    "    return tf.reduce_mean(tf.square(probs - oh), axis=-1)\n",
    "\n",
    "def sym_kl(p, q, eps=1e-7):\n",
    "    p = tf.clip_by_value(p, eps, 1.0)\n",
    "    q = tf.clip_by_value(q, eps, 1.0)\n",
    "    kl1 = tf.reduce_sum(p * tf.math.log(p/q), axis=-1)\n",
    "    kl2 = tf.reduce_sum(q * tf.math.log(q/p), axis=-1)\n",
    "    return 0.5*(kl1+kl2)\n",
    "\n",
    "# tabelas EMA\n",
    "u_mean, u_var = {}, {}\n",
    "i_mean, i_var = {}, {}\n",
    "\n",
    "def ema_update(mean_dict, var_dict, sid, val, alpha=0.9):\n",
    "    m = mean_dict.get(sid, val)\n",
    "    v = var_dict.get(sid, 0.0)\n",
    "    new_m = alpha*m + (1.0-alpha)*val\n",
    "    new_v = alpha*v + (1.0-alpha)*(val - new_m)**2\n",
    "    mean_dict[sid] = float(new_m)\n",
    "    var_dict[sid]  = float(new_v)\n",
    "\n",
    "def ucb(mu, var, c=1.0):\n",
    "    return mu + c * math.sqrt(max(var, 1e-12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7668c7",
   "metadata": {},
   "source": [
    "### Passos de treino (supervisionado e n√£o-rotulado) ‚Äî sem dupla normaliza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71c2f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_supervised(batch):\n",
    "    x_l, y_l, _ = batch\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits_l = model(x_l, training=True)  # Rescaling acontece no modelo\n",
    "        loss_sup = loss_obj(y_l, logits_l)\n",
    "    grads = tape.gradient(loss_sup, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits_l, -1), tf.cast(y_l, tf.int64)), tf.float32))\n",
    "    return loss_sup, acc\n",
    "\n",
    "def lambda_u_warmup(epoch, total=3, max_val=1.0):\n",
    "    return max_val * min(1.0, (epoch+1)/total)\n",
    "\n",
    "@tf.function\n",
    "def train_step_unlabeled(batch, lambda_u=1.0):\n",
    "    x_w, x_s, sids = batch\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits_w = model(x_w, training=True)\n",
    "        logits_s = model(x_s, training=True)\n",
    "        probs_w  = softmax_logits(logits_w)\n",
    "        probs_s  = softmax_logits(logits_s)\n",
    "\n",
    "        conf = tf.reduce_max(probs_w, axis=-1)\n",
    "        yhat = tf.argmax(probs_w, axis=-1)\n",
    "        mask = conf >= tau\n",
    "\n",
    "        loss_unsup = tf.constant(0., dtype=tf.float32)\n",
    "        masked = tf.where(mask)\n",
    "        if tf.shape(masked)[0] > 0:\n",
    "            logits_s_mask = tf.gather(logits_s, masked[:,0])\n",
    "            yhat_mask     = tf.gather(yhat, masked[:,0])\n",
    "            loss_unsup = loss_obj(yhat_mask, logits_s_mask)\n",
    "\n",
    "        loss = lambda_u * loss_unsup\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    u_batch = pseudo_el2n(probs_w)\n",
    "    i_batch = sym_kl(probs_w, probs_s)\n",
    "    return loss, u_batch, i_batch, sids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e9c674",
   "metadata": {},
   "source": [
    "### Avalia√ß√£o e execu√ß√£o de uma rodada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baa562cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_ds):\n",
    "    tot, ok, loss = 0, 0, 0.0\n",
    "    for x,y in val_ds:\n",
    "        logits = model(x, training=False)\n",
    "        loss += float(loss_obj(y, logits))\n",
    "        ok += int((tf.argmax(logits,-1).numpy() == y.numpy()).sum())\n",
    "        tot += y.shape[0]\n",
    "    return loss/max(len(list(val_ds)) or 1,1), ok/max(tot,1)\n",
    "\n",
    "def run_one_round(L_ds, U_ds, epoch_idx, epochs=5):\n",
    "    for ep in range(epochs):\n",
    "        # supervisionado\n",
    "        for batch in L_ds:\n",
    "            _ = train_step_supervised(batch)\n",
    "        # n√£o-rotulado\n",
    "        lam = lambda_u_warmup(ep, total=3, max_val=lambda_u_max)\n",
    "        for batch in U_ds:\n",
    "            loss_unsup, u_batch, i_batch, sids = train_step_unlabeled(batch, lambda_u=lam)\n",
    "\n",
    "            # atualiza EMA/UCB (use .numpy().item() p/ evitar erro de scalar)\n",
    "            u_vals = u_batch.numpy()\n",
    "            i_vals = i_batch.numpy()\n",
    "            sid_vals = sids.numpy()\n",
    "            for val_u, val_i, sid in zip(u_vals, i_vals, sid_vals):\n",
    "                ema_update(u_mean, u_var, int(sid), float(np.asarray(val_u).item()), alpha)\n",
    "                ema_update(i_mean, i_var, int(sid), float(np.asarray(val_i).item()), alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82b14a2",
   "metadata": {},
   "source": [
    "### Sele√ß√£o, simula√ß√£o de anota√ß√£o e loop ASSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb5e0e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Adicionado ao PYTHONPATH: C:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\ActiveSemisupervisedLearningCNN-s\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve() # Sobe um n√≠vel e resolve o caminho absoluto\n",
    "\n",
    "# 2. Adiciona o caminho da raiz ao sys.path\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "    print(f\"‚úÖ Adicionado ao PYTHONPATH: {PROJECT_ROOT}\")\n",
    "else:\n",
    "    print(f\"üìÅ Pasta raiz j√° est√° no PYTHONPATH: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbff35b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Rodada 1/10 ==\n",
      "Val loss: 14.1594  Val acc: 0.5002\n",
      "== Rodada 2/10 ==\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ROUNDS):\n\u001b[32m     83\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m== Rodada \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mROUNDS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ==\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[43mrun_one_round\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mU_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS_PER_ROUND\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m     \u001b[38;5;66;03m# avalia√ß√£o\u001b[39;00m\n\u001b[32m     87\u001b[39m     vloss, vacc = evaluate(model, val_ds)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mrun_one_round\u001b[39m\u001b[34m(L_ds, U_ds, epoch_idx, epochs)\u001b[39m\n\u001b[32m     16\u001b[39m lam = lambda_u_warmup(ep, total=\u001b[32m3\u001b[39m, max_val=lambda_u_max)\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m U_ds:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     loss_unsup, u_batch, i_batch, sids = \u001b[43mtrain_step_unlabeled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_u\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# atualiza EMA/UCB (use .numpy().item() p/ evitar erro de scalar)\u001b[39;00m\n\u001b[32m     21\u001b[39m     u_vals = u_batch.numpy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\ActiveSemisupervisedLearningCNN-s\\tf_env\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\ActiveSemisupervisedLearningCNN-s\\tf_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\ActiveSemisupervisedLearningCNN-s\\tf_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\ActiveSemisupervisedLearningCNN-s\\tf_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\ActiveSemisupervisedLearningCNN-s\\tf_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\ActiveSemisupervisedLearningCNN-s\\tf_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\ActiveSemisupervisedLearningCNN-s\\tf_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\ActiveSemisupervisedLearningCNN-s\\tf_env\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\ActiveSemisupervisedLearningCNN-s\\tf_env\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def acquire_topK_balanced(U_records, K, class_names=CLASS_NAMES):\n",
    "    per_class = max(1, K // max(1, len(class_names)))\n",
    "    buckets = {name: [] for name in class_names}\n",
    "\n",
    "    for rec in U_records:\n",
    "        sid = rec[\"sid\"]\n",
    "        p = rec[\"path\"].replace(\"\\\\\",\"/\").lower()\n",
    "        cname = None\n",
    "        for name in class_names:\n",
    "            if f\"/{name.lower()}/\" in p:\n",
    "                cname = name; break\n",
    "        if cname is None: \n",
    "            continue\n",
    "        mu_u, vu_u = u_mean.get(sid, 0.0), u_var.get(sid, 0.0)\n",
    "        mu_i, vi_i = i_mean.get(sid, 0.0), i_var.get(sid, 0.0)\n",
    "        score = ucb(mu_u, vu_u, c_u) * ucb(mu_i, vi_i, c_i)\n",
    "        buckets[cname].append((score, sid))\n",
    "\n",
    "    selected = []\n",
    "    for cname in class_names:\n",
    "        cand = sorted(buckets[cname], key=lambda x: x[0], reverse=True)\n",
    "        selected.extend([sid for _, sid in cand[:per_class]])\n",
    "\n",
    "    if len(selected) < K:\n",
    "        rest = []\n",
    "        have = set(selected)\n",
    "        for rec in U_records:\n",
    "            sid = rec[\"sid\"]\n",
    "            if sid in have: continue\n",
    "            mu_u, vu_u = u_mean.get(sid, 0.0), u_var.get(sid, 0.0)\n",
    "            mu_i, vi_i = i_mean.get(sid, 0.0), i_var.get(sid, 0.0)\n",
    "            rest.append((ucb(mu_u,vu_u,c_u)*ucb(mu_i,vi_i,c_i), sid))\n",
    "        rest.sort(key=lambda x: x[0], reverse=True)\n",
    "        selected.extend([sid for _, sid in rest[:K-len(selected)]])\n",
    "    return selected[:K]\n",
    "\n",
    "def simulate_sid2true_from_path(U_records, L_records, class_map=CLASS_MAP):\n",
    "    sid2true = {}\n",
    "    def infer(path):\n",
    "        p = path.replace(\"\\\\\",\"/\").lower()\n",
    "        for name, idx in class_map.items():\n",
    "            if f\"/{name.lower()}/\" in p:\n",
    "                return idx\n",
    "        raise ValueError(f\"N√£o infere classe de: {path}\")\n",
    "    for rec in U_records: sid2true[rec[\"sid\"]] = infer(rec[\"path\"])\n",
    "    for rec in L_records: sid2true[rec[\"sid\"]] = infer(rec[\"path\"])\n",
    "    return sid2true\n",
    "\n",
    "def move_annotated_to_L(sid2label, U_records, L_records):\n",
    "    by_id = {r[\"sid\"]: r for r in U_records}\n",
    "    moved = []\n",
    "    keepU = []\n",
    "    for r in U_records:\n",
    "        sid = r[\"sid\"]\n",
    "        if sid in sid2label:\n",
    "            nr = dict(r)\n",
    "            nr[\"label\"] = int(sid2label[sid])\n",
    "            moved.append(nr)\n",
    "        else:\n",
    "            keepU.append(r)\n",
    "    return keepU, (L_records + moved)\n",
    "\n",
    "# ==== Dataset base ====\n",
    "train_root = r\"..\\data\\train\"\n",
    "val_root   = r\"..\\data\\validation\"\n",
    "\n",
    "all_train = build_records_from_dir(train_root, class_map=CLASS_MAP)\n",
    "L_records, U_records = split_L_U(all_train, n_L=400, seed=42)\n",
    "\n",
    "val_records = build_records_from_dir(val_root, class_map=CLASS_MAP)\n",
    "val_ds = make_val_ds(val_records, batch=BATCH, image_size=IMAGE_SIZE)\n",
    "\n",
    "L_ds = make_L_ds(L_records, batch=BATCH, image_size=IMAGE_SIZE)\n",
    "U_ds = make_U_ds(U_records, batch=BATCH, image_size=IMAGE_SIZE)\n",
    "\n",
    "# simula√ß√£o de anota√ß√£o (sem arquivos)\n",
    "sid2true = simulate_sid2true_from_path(U_records, L_records, class_map=CLASS_MAP)\n",
    "\n",
    "ROUNDS = 10\n",
    "EPOCHS_PER_ROUND = 5\n",
    "\n",
    "for r in range(ROUNDS):\n",
    "    print(f\"== Rodada {r+1}/{ROUNDS} ==\")\n",
    "    run_one_round(L_ds, U_ds, epoch_idx=r, epochs=EPOCHS_PER_ROUND)\n",
    "\n",
    "    # avalia√ß√£o\n",
    "    vloss, vacc = evaluate(model, val_ds)\n",
    "    print(f\"Val loss: {vloss:.4f}  Val acc: {vacc:.4f}\")\n",
    "\n",
    "    # sele√ß√£o ativa balanceada\n",
    "    selected_ids = acquire_topK_balanced(U_records, K=100, class_names=CLASS_NAMES)\n",
    "\n",
    "    # ‚Äúanota√ß√£o‚Äù simulada apenas nos K escolhidos\n",
    "    sid2label = {sid: sid2true[sid] for sid in selected_ids}\n",
    "    # mover U->L\n",
    "    U_records, L_records = move_annotated_to_L(sid2label, U_records, L_records)\n",
    "\n",
    "    # limpar EMA/UCB dos sids movidos\n",
    "    moved_set = set(sid2label.keys())\n",
    "    for sid in list(u_mean.keys()):\n",
    "        if sid in moved_set: u_mean.pop(sid, None); u_var.pop(sid, None)\n",
    "    for sid in list(i_mean.keys()):\n",
    "        if sid in moved_set: i_mean.pop(sid, None); i_var.pop(sid, None)\n",
    "\n",
    "    # rebuild datasets\n",
    "    L_ds = make_L_ds(L_records, batch=BATCH, image_size=IMAGE_SIZE)\n",
    "    U_ds = make_U_ds(U_records, batch=BATCH, image_size=IMAGE_SIZE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
