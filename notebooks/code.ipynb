{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25863e7a",
   "metadata": {},
   "source": [
    "### Config (dimens√µes, batch, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "266bfc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, json, math, random, pathlib\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# ==== CONFIG ====\n",
    "H, W = 180, 180\n",
    "IMAGE_SIZE = (H, W)\n",
    "BATCH = 64\n",
    "NUM_CLASSES = 2\n",
    "CLASS_NAMES = (\"Cat\", \"Dog\")   # nomes como est√£o nas pastas\n",
    "CLASS_MAP = {name: i for i, name in enumerate(CLASS_NAMES)}\n",
    "INV_CLASS_MAP = {v:k for k,v in CLASS_MAP.items()}\n",
    "MAX_U_STEPS = 500 # ex.: 500 para limitar passos U/√©poca; None = todos\n",
    "DEBUG = True                  # True = logs verbosos, sem @tf.function no passo U\n",
    "SHUFFLE_BUF_L = 1024          # buffers menores evitam longas ‚Äúfilling up‚Ä¶‚Äù\n",
    "SHUFFLE_BUF_U = 1024\n",
    "PREFETCH = tf.data.AUTOTUNE\n",
    "LOG_EVERY_L = 50              # log a cada N batches L\n",
    "LOG_EVERY_U = 50  \n",
    "\n",
    "tau = 0.7          # threshold inicial mais brando\n",
    "lambda_u_max = 1.0 # peso m√°ximo da perda unsupervisionada\n",
    "c_u, c_i = 1.0, 1.0  # coef. UCB\n",
    "alpha = 0.9          # EMA\n",
    "\n",
    "tf.config.threading.set_inter_op_parallelism_threads(0)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3da6b4",
   "metadata": {},
   "source": [
    "### Helpers de leitura e record builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6bc9065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_decode_resize_jpeg(path, image_size):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3, dct_method='INTEGER_ACCURATE')\n",
    "    img = tf.image.resize(img, image_size, antialias=True)\n",
    "    img = tf.cast(img, tf.float32) / 255.0  # [0,1] ‚Äî √öNICA normaliza√ß√£o\n",
    "    return img\n",
    "\n",
    "IGNORE_ERRORS = tf.data.experimental.ignore_errors()\n",
    "\n",
    "def build_records_from_dir(root_dir, class_map=CLASS_MAP, with_ids=True, seed=42):\n",
    "    \"\"\"\n",
    "    Varre .../Cat/*.jpg e .../Dog/*.jpg gerando records {sid, path, label}\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    records = []\n",
    "    sid = 0\n",
    "    for name, idx in class_map.items():\n",
    "        cls_dir = os.path.join(root_dir, name)\n",
    "        for fn in os.listdir(cls_dir):\n",
    "            if not fn.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                continue\n",
    "            p = os.path.join(cls_dir, fn)\n",
    "            rec = {\"sid\": sid, \"path\": p}\n",
    "            if class_map is not None:\n",
    "                rec[\"label\"] = idx\n",
    "            records.append(rec)\n",
    "            sid += 1\n",
    "    rng.shuffle(records)\n",
    "    return records\n",
    "\n",
    "def split_L_U(records, n_L=400, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    arr = records.copy()\n",
    "    rng.shuffle(arr)\n",
    "    L = []\n",
    "    U = []\n",
    "    for r in arr:\n",
    "        if len(L) < n_L:\n",
    "            L.append(r)\n",
    "        else:\n",
    "            U.append(r)\n",
    "    return L, U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4df7e6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_decode_resize_jpeg(path, image_size):\n",
    "    # path √© tf.Tensor (scalar string)\n",
    "    img_bytes = tf.io.read_file(path)\n",
    "\n",
    "    # decode robusto a JPEG com headers estranhos\n",
    "    img = tf.image.decode_jpeg(img_bytes, channels=3, dct_method='INTEGER_ACCURATE')\n",
    "\n",
    "    # resize com antialias\n",
    "    img = tf.image.resize(img, image_size, antialias=True)\n",
    "\n",
    "    # cast e normaliza√ß√£o AQUI (op√ß√£o B)\n",
    "    img = tf.cast(img, tf.float32) / 255.0  # <- garante [0,1]\n",
    "\n",
    "    # (opcional) clip de seguran√ßa por ru√≠do/artefatos\n",
    "    img = tf.clip_by_value(img, 0.0, 1.0)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832d6a60",
   "metadata": {},
   "source": [
    "### Augmentations batch-first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "943cd0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_aug_seq = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomTranslation(0.05, 0.05),\n",
    "], name=\"weak_aug\")\n",
    "\n",
    "strong_aug_seq = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.15),\n",
    "    layers.RandomZoom(0.15),\n",
    "    layers.RandomContrast(0.15),\n",
    "], name=\"strong_aug\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd06804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== AUGMENTS ================================================================\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "IMAGE_SIZE = (180, 180)  # ajuste ao seu valor\n",
    "\n",
    "def make_weak_aug():\n",
    "    # Flip + pequeno crop ‚Üí mant√©m sem√¢ntica (FixMatch \"weak\")\n",
    "    return keras.Sequential([\n",
    "        # keras.layers.RandomFlip(\"horizontal\"),\n",
    "        # keras.layers.RandomTranslation(height_factor=0.02, width_factor=0.02, fill_mode=\"reflect\"),\n",
    "        keras.layers.RandomFlip(\"horizontal\"),\n",
    "        keras.layers.RandomRotation(0.05),\n",
    "        keras.layers.RandomTranslation(height_factor=0.02, width_factor=0.02, fill_mode=\"reflect\"),\n",
    "        layers.Lambda(lambda x: tf.clip_by_value(x, 0.0, 1.0)),\n",
    "    ])\n",
    "\n",
    "def make_strong_aug(mild=False):\n",
    "    if mild:\n",
    "        # primeiras √©pocas: menos agressivo\n",
    "        return keras.Sequential([\n",
    "            keras.layers.RandomFlip(\"horizontal\"),\n",
    "            keras.layers.RandomRotation(0.15),\n",
    "            keras.layers.RandomContrast(0.2),\n",
    "            keras.layers.Lambda(lambda x: tf.clip_by_value(x, 0.0, 1.0)),\n",
    "        ])\n",
    "    # depois: mais agressivo\n",
    "    return keras.Sequential([\n",
    "        keras.layers.RandomFlip(\"horizontal\"),\n",
    "        keras.layers.RandomRotation(0.25),\n",
    "        keras.layers.RandomContrast(0.4),\n",
    "        keras.layers.RandomBrightness(factor=0.2),\n",
    "        keras.layers.Lambda(lambda x: tf.clip_by_value(x, 0.0, 1.0)),\n",
    "    ])\n",
    "\n",
    "weak_aug   = make_weak_aug()\n",
    "strong_aug = make_strong_aug(mild=True)   # come√ßamos \"mild\"; trocaremos no loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ebf26d",
   "metadata": {},
   "source": [
    "### Datasets (vers√µes √∫nicas, sem duplicidade de batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e25a4f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_ERRORS = tf.data.experimental.ignore_errors()\n",
    "\n",
    "def make_L_ds(L_records, batch, image_size, drop_remainder=False):\n",
    "    paths  = [r[\"path\"] for r in L_records]\n",
    "    labels = [r[\"label\"] for r in L_records]\n",
    "    ids    = [r[\"sid\"]   for r in L_records]\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels, ids))\n",
    "\n",
    "    def _load(path, y, sid):\n",
    "        img = _safe_decode_resize_jpeg(path, image_size)  # [0,1]\n",
    "        return img, y, sid\n",
    "\n",
    "    ds = ds.shuffle(SHUFFLE_BUF_L, reshuffle_each_iteration=True)\n",
    "    ds = ds.map(_load, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    ds = ds.apply(IGNORE_ERRORS)\n",
    "    ds = ds.batch(batch, drop_remainder=drop_remainder)\n",
    "    ds = ds.prefetch(PREFETCH)\n",
    "    return ds\n",
    "\n",
    "def make_U_ds(U_records, batch, image_size, drop_remainder=False,\n",
    "              weak_aug=None, strong_aug=None):\n",
    "    paths = [r[\"path\"] for r in U_records]\n",
    "    ids   = [r[\"sid\"]  for r in U_records]\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, ids))\n",
    "\n",
    "    def _loadU(path, sid):\n",
    "        img = _safe_decode_resize_jpeg(path, image_size)  # [0,1]\n",
    "        return img, sid\n",
    "\n",
    "    ds = ds.shuffle(SHUFFLE_BUF_U, reshuffle_each_iteration=True)\n",
    "    ds = ds.map(_loadU, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    ds = ds.apply(IGNORE_ERRORS)\n",
    "    ds = ds.batch(batch, drop_remainder=drop_remainder)\n",
    "\n",
    "    # aplica augment AP√ìS o batch\n",
    "    def _make_pair(batch_img, batch_sid):\n",
    "        x_w = weak_aug_seq(batch_img, training=True)\n",
    "        x_s = strong_aug_seq(batch_img, training=True)\n",
    "        # redundante se j√° h√° Lambda nas seqs, mas seguro:\n",
    "        x_w = tf.clip_by_value(x_w, 0.0, 1.0)\n",
    "        x_s = tf.clip_by_value(x_s, 0.0, 1.0)\n",
    "        return x_w, x_s, batch_sid\n",
    "\n",
    "    ds = ds.map(_make_pair, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    ds = ds.prefetch(PREFETCH)\n",
    "    return ds\n",
    "\n",
    "def make_val_ds(val_records, batch, image_size):\n",
    "    paths  = [r[\"path\"] for r in val_records]\n",
    "    labels = [r[\"label\"] for r in val_records]\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "\n",
    "    def _load(path, y):\n",
    "        img = _safe_decode_resize_jpeg(path, image_size)  # [0,1]\n",
    "        return img, y\n",
    "\n",
    "    ds = ds.map(_load, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c180f",
   "metadata": {},
   "source": [
    "### Modelo √∫nico com normaliza√ß√£o no modelo (evita duplicidade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f9f825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes=NUM_CLASSES, image_size=IMAGE_SIZE):\n",
    "    inp = keras.Input(shape=image_size + (3,))\n",
    "    x = layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    out = layers.Dense(num_classes)(x)  # logits\n",
    "    return keras.Model(inp, out)\n",
    "\n",
    "model = build_model()\n",
    "optimizer = keras.optimizers.Adam(1e-3)\n",
    "loss_obj  = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d0811a",
   "metadata": {},
   "source": [
    "### M√©tricas por amostra, EMA/UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6535e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_logits(logits):\n",
    "    return tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "def pseudo_el2n(probs):  # incerteza (EL2N-like, proxy simples)\n",
    "    # dist√¢ncia do one-hot do argmax\n",
    "    yhat = tf.argmax(probs, axis=-1)\n",
    "    oh = tf.one_hot(yhat, depth=tf.shape(probs)[-1])\n",
    "    return tf.reduce_mean(tf.square(probs - oh), axis=-1)\n",
    "\n",
    "def sym_kl(p, q, eps=1e-7):\n",
    "    p = tf.clip_by_value(p, eps, 1.0)\n",
    "    q = tf.clip_by_value(q, eps, 1.0)\n",
    "    kl1 = tf.reduce_sum(p * tf.math.log(p/q), axis=-1)\n",
    "    kl2 = tf.reduce_sum(q * tf.math.log(q/p), axis=-1)\n",
    "    return 0.5*(kl1+kl2)\n",
    "\n",
    "# tabelas EMA\n",
    "u_mean, u_var = {}, {}\n",
    "i_mean, i_var = {}, {}\n",
    "\n",
    "# tabelas EMA (GLOBAIS): renomeadas para evitar colis√£o\n",
    "U_MEAN, U_VAR = {}, {}\n",
    "I_MEAN, I_VAR = {}, {}\n",
    "\n",
    "def ema_update(mean_dict, var_dict, sid, val, alpha=0.9):\n",
    "    sid = int(sid)\n",
    "    val = float(val)\n",
    "    m = mean_dict.get(sid, val)\n",
    "    v = var_dict.get(sid, 0.0)\n",
    "    new_m = alpha*m + (1.0-alpha)*val\n",
    "    new_v = alpha*v + (1.0-alpha)*(val - new_m)**2\n",
    "    mean_dict[sid] = float(new_m)\n",
    "    var_dict[sid]  = float(new_v)\n",
    "\n",
    "def ucb(mu, var, c=1.0):\n",
    "    return mu + c * math.sqrt(max(var, 1e-12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87e2e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== SCHEDULES ==============================================================\n",
    "def tau_schedule(epoch):\n",
    "    # 0‚Üí0.6, 1‚Üí0.7, 2‚Üí0.8, saturando em 0.95 (se quiser, aumente depois)\n",
    "    return min(0.95, 0.6 + 0.1*epoch)\n",
    "\n",
    "def lambda_u_warmup(epoch, total=3, max_val=1.0):\n",
    "    # sobe r√°pido para dar peso ao U cedo\n",
    "    return max_val * min(1.0, (epoch+1)/total)\n",
    "\n",
    "def choose_adaptive_tau(conf_list, target_rate=0.6, \n",
    "                        tau_min=0.40, tau_max=0.95,\n",
    "                        prev_tau=None, smooth=0.6):\n",
    "    \"\"\"\n",
    "    conf_list: lista/np.array de confian√ßas (max softmax da weak)\n",
    "    target_rate: fra√ß√£o de amostras que devem PASSAR na m√°scara (ex.: 0.6 -> 60%)\n",
    "    tau_min/tau_max: limites de seguran√ßa\n",
    "    prev_tau: tau da √©poca anterior (para suaviza√ß√£o exponencial)\n",
    "    smooth: fator de suaviza√ß√£o (0..1). 1.0 = sem suaviza√ß√£o\n",
    "    \"\"\"\n",
    "    if len(conf_list) == 0:\n",
    "        # fallback conservador\n",
    "        base = 0.5\n",
    "        return float(prev_tau if prev_tau is not None else base)\n",
    "\n",
    "    # Queremos que ~target_rate passem: tau = quantil de (1 - target_rate)\n",
    "    q = float(np.clip(1.0 - target_rate, 0.0, 1.0))\n",
    "    raw_tau = float(np.quantile(np.asarray(conf_list), q))\n",
    "    raw_tau = float(np.clip(raw_tau, tau_min, tau_max))\n",
    "\n",
    "    if prev_tau is None:\n",
    "        return raw_tau\n",
    "    # suaviza√ß√£o exponencial: mais est√°vel entre √©pocas\n",
    "    return float(smooth * prev_tau + (1.0 - smooth) * raw_tau)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59413b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probe_confidences_on_U(model, U_ds, temp_T=0.5, \n",
    "                           max_batches=32,  # amostra parcial p/ ser r√°pido\n",
    "                           stop_if_n=8192   # ou para se j√° coletou muitas confs\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Faz forward em alguns batches de U para medir confian√ßas da WEAK.\n",
    "    Retorna uma lista de confian√ßas (max softmax).\n",
    "    \"\"\"\n",
    "    confs = []\n",
    "    n_batches = 0\n",
    "    for x_w, _, _ in U_ds:  # U_ds -> (x_w, x_s, sid)\n",
    "        logits_w = model(x_w, training=False)\n",
    "        probs_w  = tf.nn.softmax(logits_w / temp_T, axis=-1)\n",
    "        conf     = tf.reduce_max(probs_w, axis=-1).numpy()  # (B,)\n",
    "        confs.append(conf)\n",
    "        n_batches += 1\n",
    "        if n_batches >= max_batches or sum(len(c) for c in confs) >= stop_if_n:\n",
    "            break\n",
    "    if len(confs) == 0:\n",
    "        return []\n",
    "    return np.concatenate(confs, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7668c7",
   "metadata": {},
   "source": [
    "### Passos de treino (supervisionado e n√£o-rotulado) ‚Äî sem dupla normaliza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71c2f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_supervised(batch):\n",
    "    x_l, y_l, _ = batch\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits_l = model(x_l, training=True)  # Rescaling acontece no modelo\n",
    "        loss_sup = loss_obj(y_l, logits_l)\n",
    "    grads = tape.gradient(loss_sup, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(logits_l, -1), tf.cast(y_l, tf.int64)), tf.float32)\n",
    "    )\n",
    "    return loss_sup, acc\n",
    "\n",
    "def train_step_unlabeled_with_masklog(batch, lambda_u=1.0, tau=0.7, temp_T=0.5):\n",
    "    x_w, x_s, sids = batch\n",
    "\n",
    "    # ---------- forward (fora do tape) para m√°scara e m√©tricas ----------\n",
    "    logits_w = model(x_w, training=True)\n",
    "    probs_w  = tf.nn.softmax(logits_w / temp_T, axis=-1)\n",
    "\n",
    "    # m√©tricas (incerteza U, inconsist√™ncia I) e m√°scara\n",
    "    conf = tf.reduce_max(probs_w, axis=-1)           # [B]\n",
    "    yhat = tf.argmax(probs_w, axis=-1)               # [B], int\n",
    "    mask = conf >= tau                               # [B], bool\n",
    "    masked = tf.where(mask)                          # [M,1]\n",
    "    masked_count = tf.shape(masked)[0]\n",
    "\n",
    "    # Consist√™ncia com strong (para logging apenas ‚Äì fora do tape)\n",
    "    logits_s_for_I = model(x_s, training=True)\n",
    "    probs_s_for_I  = tf.nn.softmax(logits_s_for_I, axis=-1)\n",
    "\n",
    "    u_batch = pseudo_el2n(probs_w)                   # [B]\n",
    "    i_batch = sym_kl(probs_w, probs_s_for_I)         # [B]\n",
    "\n",
    "    # Sem exemplos v√°lidos ou sem peso ‚Üí n√£o atualiza\n",
    "    if (lambda_u <= 0.0) or tf.equal(masked_count, 0):\n",
    "        return {\n",
    "            \"loss\": tf.constant(0.0, tf.float32),\n",
    "            \"u_batch\": u_batch,\n",
    "            \"i_batch\": i_batch,\n",
    "            \"sids\": sids,\n",
    "            \"mask_pass\": tf.reduce_sum(tf.cast(mask, tf.int32)),\n",
    "            \"mask_total\": tf.size(mask),\n",
    "            \"skipped\": True,\n",
    "        }\n",
    "\n",
    "    # ---------- forward DENTRO do tape (grava computa√ß√£o para gradiente) ----------\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits_s = model(x_s, training=True)                   # recomputa dentro do tape\n",
    "        logits_s_mask = tf.gather(logits_s, masked[:, 0])      # [M, C]\n",
    "        yhat_mask     = tf.gather(yhat,     masked[:, 0])      # [M]\n",
    "        # perda de consist√™ncia (pseudo-r√≥tulo)\n",
    "        loss_unsup = loss_obj(yhat_mask, logits_s_mask)        # scalar\n",
    "        loss = lambda_u * loss_unsup\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # Filtra gradientes None\n",
    "    grads_vars = [(g, v) for g, v in zip(grads, model.trainable_variables) if g is not None]\n",
    "    if not grads_vars:\n",
    "        return {\n",
    "            \"loss\": tf.constant(0.0, tf.float32),\n",
    "            \"u_batch\": u_batch,\n",
    "            \"i_batch\": i_batch,\n",
    "            \"sids\": sids,\n",
    "            \"mask_pass\": tf.reduce_sum(tf.cast(mask, tf.int32)),\n",
    "            \"mask_total\": tf.size(mask),\n",
    "            \"skipped\": True,\n",
    "        }\n",
    "\n",
    "    optimizer.apply_gradients(grads_vars)\n",
    "\n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"u_batch\": u_batch,\n",
    "        \"i_batch\": i_batch,\n",
    "        \"sids\": sids,\n",
    "        \"mask_pass\": tf.reduce_sum(tf.cast(mask, tf.int32)),\n",
    "        \"mask_total\": tf.size(mask),\n",
    "        \"skipped\": False,\n",
    "    }\n",
    "\n",
    "\n",
    "def lambda_u_warmup(epoch, total=3, max_val=1.0):\n",
    "    return max_val * min(1.0, (epoch+1)/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e9c674",
   "metadata": {},
   "source": [
    "### Avalia√ß√£o e execu√ß√£o de uma rodada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baa562cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_ds):\n",
    "    tot, ok, loss_sum, n_batches = 0, 0, 0.0, 0\n",
    "    for x, y in val_ds:\n",
    "        logits = model(x, training=False)\n",
    "        loss_sum += float(loss_obj(y, logits))\n",
    "        ok += int((tf.argmax(logits, -1).numpy() == y.numpy()).sum())\n",
    "        tot += int(y.shape[0])\n",
    "        n_batches += 1\n",
    "    mean_loss = loss_sum / max(n_batches, 1)\n",
    "    acc = ok / max(tot, 1)\n",
    "    return mean_loss, acc\n",
    "\n",
    "# ==== LOOP DE UMA RODADA =====================================================\n",
    "# def run_one_round(L_ds, U_ds, epoch_idx, epochs=5, lambda_u_max=1.0, temp_T=0.5):\n",
    "#     import time\n",
    "#     for ep in range(epochs):\n",
    "#         current_tau = tau_schedule(ep)\n",
    "#         lam = lambda_u_warmup(ep, total=3, max_val=lambda_u_max)\n",
    "\n",
    "#         # (Opcional) logging r√°pido de confian√ßa do 1¬∫ batch de U\n",
    "#         try:\n",
    "#             first_u = next(iter(U_ds))\n",
    "#             xw, xs, _ = first_u\n",
    "#             pw = tf.nn.softmax(model(xw, training=False) / temp_T, axis=-1)\n",
    "#             conf = tf.reduce_max(pw, axis=-1).numpy()\n",
    "#             conf_mean = float(conf.mean())\n",
    "#             conf_p90  = float(np.percentile(conf, 90))\n",
    "#         except StopIteration:\n",
    "#             conf_mean, conf_p90 = float('nan'), float('nan')\n",
    "\n",
    "#         print(f\"[epoch {epoch_idx}:{ep+1}/{epochs}]  tau={current_tau:.2f}  Œª_u={lam:.2f}  T={temp_T:.2f}  confŒº={conf_mean:.3f}  confP90={conf_p90:.3f}\")\n",
    "\n",
    "#         # ---------- supervisionado ----------\n",
    "#         tL = time.time()\n",
    "#         L_batches, L_loss_sum, L_acc_sum = 0, 0.0, 0.0\n",
    "#         for batch in L_ds:\n",
    "#             loss_sup, acc = train_step_supervised(batch)\n",
    "#             L_loss_sum += float(loss_sup)\n",
    "#             L_acc_sum  += float(acc)\n",
    "#             L_batches += 1\n",
    "#         if L_batches:\n",
    "#             print(f\"  L | batches={L_batches}  loss~={L_loss_sum/L_batches:.4f}  acc~={L_acc_sum/L_batches:.3f}  ({time.time()-tL:.1f}s)\")\n",
    "\n",
    "#         # ---------- n√£o-rotulado ----------\n",
    "#         tU = time.time()\n",
    "#         U_batches = 0\n",
    "#         mask_pass_sum = 0\n",
    "#         mask_total_sum = 0\n",
    "#         skipped_batches = 0\n",
    "#         U_u_sum = 0.0\n",
    "#         U_i_sum = 0.0\n",
    "\n",
    "#         for batch in U_ds:\n",
    "#             out = train_step_unlabeled_with_masklog(batch, lambda_u=lam, tau=current_tau, T=temp_T)\n",
    "\n",
    "#             U_batches += 1\n",
    "#             mask_pass_sum  += int(out[\"mask_pass\"].numpy())\n",
    "#             mask_total_sum += int(out[\"mask_total\"].numpy())\n",
    "#             skipped_batches += int(bool(out[\"skipped\"]))\n",
    "#             U_u_sum += float(tf.reduce_mean(out[\"u_batch\"]).numpy())\n",
    "#             U_i_sum += float(tf.reduce_mean(out[\"i_batch\"]).numpy())\n",
    "\n",
    "#         if mask_total_sum > 0:\n",
    "#             mask_rate = mask_pass_sum / mask_total_sum * 100.0\n",
    "#         else:\n",
    "#             mask_rate = 0.0\n",
    "\n",
    "#         print(f\"  U | batches={U_batches}  mask_rate={mask_rate:.1f}%  skipped={skipped_batches}  U~={U_u_sum/max(U_batches,1):.4f}  I~={U_i_sum/max(U_batches,1):.4f}  ({time.time()-tU:.1f}s)\")\n",
    "\n",
    "def run_one_round(L_ds, U_ds, epoch_idx, epochs=5, \n",
    "                  lambda_u_max=1.0, \n",
    "                  base_tau=0.60, \n",
    "                  temp_T=0.5,\n",
    "                  target_rate_mild=0.60,   # ~60% passam na fase mild\n",
    "                  target_rate_strong=0.50, # ~50% na fase forte\n",
    "                  is_strong_phase=False,    # defina True/False fora conforme sua agenda mild‚Üístrong\n",
    "                  prev_tau=None):           # opcional: passa o tau da √©poca anterior p/ suavizar\n",
    "    \"\"\"\n",
    "    Retorna o √∫ltimo tau usado (para voc√™ reutilizar/suavizar na pr√≥xima chamada).\n",
    "    \"\"\"\n",
    "    tau_used = prev_tau if prev_tau is not None else base_tau\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        # ---- 1) Supervisionado (igual)\n",
    "        tL0 = time.time()\n",
    "        l_loss_sum, l_acc_sum, l_batches = 0.0, 0.0, 0\n",
    "        for batch in L_ds:\n",
    "            loss_sup, acc = train_step_supervised(batch)\n",
    "            l_loss_sum += float(loss_sup); l_acc_sum += float(acc); l_batches += 1\n",
    "        tL1 = time.time()\n",
    "\n",
    "        # ---- 2) Probe confian√ßas da WEAK para escolher tau adaptativo desta √©poca\n",
    "        confs = probe_confidences_on_U(model, U_ds, temp_T=temp_T,\n",
    "                                       max_batches=32, stop_if_n=8192)\n",
    "        target_rate = (target_rate_strong if is_strong_phase else target_rate_mild)\n",
    "        tau_used = choose_adaptive_tau(confs, target_rate=target_rate, \n",
    "                                       tau_min=0.40, tau_max=0.95,\n",
    "                                       prev_tau=tau_used, smooth=0.6)\n",
    "\n",
    "        # ---- 3) N√£o-rotulado (FixMatch) com tau adaptativo\n",
    "        lam = lambda_u_warmup(ep, total=3, max_val=lambda_u_max)\n",
    "        tU0 = time.time()\n",
    "        u_batches, skipped_all = 0, 0\n",
    "        pass_sum, total_sum = 0, 0\n",
    "        u_mean_local, i_mean_local = 0.0, 0.0\n",
    "\n",
    "        for batch in U_ds:\n",
    "            out = train_step_unlabeled_with_masklog(\n",
    "                batch, lambda_u=lam, tau=tau_used, temp_T=temp_T\n",
    "            )\n",
    "            u_batches += 1\n",
    "            skipped_all += int(out[\"skipped\"])\n",
    "\n",
    "            # m√©tricas\n",
    "            pass_sum  += int(out[\"mask_pass\"].numpy())\n",
    "            total_sum += int(out[\"mask_total\"].numpy())\n",
    "            u_mean_local += float(tf.reduce_mean(out[\"u_batch\"]))\n",
    "            i_mean_local += float(tf.reduce_mean(out[\"i_batch\"]))\n",
    "\n",
    "            # EMA/UCB\n",
    "            u_vals  = out[\"u_batch\"].numpy()\n",
    "            i_vals  = out[\"i_batch\"].numpy()\n",
    "            sid_vals= out[\"sids\"].numpy()\n",
    "            for val_u, val_i, sid in zip(u_vals, i_vals, sid_vals):\n",
    "                ema_update(U_MEAN, U_VAR, int(sid), float(val_u), alpha=0.9)\n",
    "                ema_update(I_MEAN, I_VAR, int(sid), float(val_i), alpha=0.9)\n",
    "\n",
    "        tU1 = time.time()\n",
    "\n",
    "        mask_rate = (pass_sum / max(total_sum, 1)) * 100.0\n",
    "        print(f\"[epoch {epoch_idx}:{ep+1}/{epochs}]  \"\n",
    "              f\"tau={tau_used:.2f}  Œª_u={lam:.2f}  T={temp_T:.2f}  \"\n",
    "              f\"mask_rate={mask_rate:.1f}%  skipped_batches={skipped_all}/{u_batches}  \"\n",
    "              f\"L_loss~={l_loss_sum/max(l_batches,1):.4f}  L_acc~={l_acc_sum/max(l_batches,1):.3f}  \"\n",
    "              f\"U~={u_mean_local/max(u_batches,1):.4f}  I~={i_mean_local/max(u_batches,1):.4f}  \"\n",
    "              f\"({tL1-tL0:.1f}s L, {tU1-tU0:.1f}s U)\")\n",
    "\n",
    "    return tau_used\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82b14a2",
   "metadata": {},
   "source": [
    "### Sele√ß√£o, simula√ß√£o de anota√ß√£o e loop ASSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb5e0e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Adicionado ao PYTHONPATH: C:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\ActiveSemisupervisedLearningCNN-s\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve() # Sobe um n√≠vel e resolve o caminho absoluto\n",
    "\n",
    "# 2. Adiciona o caminho da raiz ao sys.path\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "    print(f\"‚úÖ Adicionado ao PYTHONPATH: {PROJECT_ROOT}\")\n",
    "else:\n",
    "    print(f\"üìÅ Pasta raiz j√° est√° no PYTHONPATH: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbff35b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L min/max: 0.0 1.0\n",
      "U weak min/max: 0.0 1.0\n",
      "U strong min/max: 0.0 1.0\n",
      "== Rodada 1/10 ==\n",
      "[epoch 0:1/2]  tau=0.58  Œª_u=0.33  T=0.50  mask_rate=99.8%  skipped_batches=0/346  L_loss~=2.3130  L_acc~=0.471  U~=0.0006  I~=0.0005  (10.7s L, 795.1s U)\n",
      "[epoch 0:2/2]  tau=0.62  Œª_u=0.67  T=0.50  mask_rate=100.0%  skipped_batches=0/346  L_loss~=15.7348  L_acc~=0.511  U~=0.0009  I~=0.0014  (6.8s L, 1092.6s U)\n",
      "[epoch 0:1/3]  tau=0.74  Œª_u=0.33  T=0.50  mask_rate=100.0%  skipped_batches=0/346  L_loss~=10.6311  L_acc~=0.491  U~=0.0000  I~=0.0005  (5.7s L, 651.4s U)\n",
      "[epoch 0:2/3]  tau=0.65  Œª_u=0.67  T=0.50  mask_rate=77.7%  skipped_batches=72/346  L_loss~=2.7079  L_acc~=0.527  U~=0.0528  I~=0.0011  (5.7s L, 559.0s U)\n",
      "[epoch 0:3/3]  tau=0.77  Œª_u=1.00  T=0.50  mask_rate=97.1%  skipped_batches=5/346  L_loss~=8.7216  L_acc~=0.513  U~=0.0044  I~=0.0016  (5.5s L, 632.0s U)\n",
      "Val loss: 54.8736  Val acc: 0.4998\n",
      "acquire_topK_balanced: elapsed 0.1147 s\n",
      "== Rodada 2/10 ==\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_4020\\1438945461.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    111\u001b[39m     strong_aug = make_strong_aug(mild=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    112\u001b[39m     U_ds = make_U_ds(U_records, batch=BATCH, image_size=IMAGE_SIZE,\n\u001b[32m    113\u001b[39m                      weak_aug=weak_aug, strong_aug=strong_aug)\n\u001b[32m    114\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     run_one_round(L_ds, U_ds, epoch_idx=r, epochs=\u001b[32m2\u001b[39m, lambda_u_max=\u001b[32m1.0\u001b[39m, temp_T=\u001b[32m0.5\u001b[39m)\n\u001b[32m    116\u001b[39m \n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m# --- depois, for√ßa total ---\u001b[39;00m\n\u001b[32m    118\u001b[39m     strong_aug = make_strong_aug(mild=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_4020\\4000415191.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(L_ds, U_ds, epoch_idx, epochs, lambda_u_max, base_tau, temp_T, target_rate_mild, target_rate_strong, is_strong_phase, prev_tau)\u001b[39m\n\u001b[32m    104\u001b[39m         pass_sum, total_sum = \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m    105\u001b[39m         u_mean_local, i_mean_local = \u001b[32m0.0\u001b[39m, \u001b[32m0.0\u001b[39m\n\u001b[32m    106\u001b[39m \n\u001b[32m    107\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;28;01min\u001b[39;00m U_ds:\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m             out = train_step_unlabeled_with_masklog(\n\u001b[32m    109\u001b[39m                 batch, lambda_u=lam, tau=tau_used, temp_T=temp_T\n\u001b[32m    110\u001b[39m             )\n\u001b[32m    111\u001b[39m             u_batches += \u001b[32m1\u001b[39m\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_4020\\690675058.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(batch, lambda_u, tau, temp_T)\u001b[39m\n\u001b[32m     52\u001b[39m         \u001b[38;5;66;03m# perda de consist√™ncia (pseudo-r√≥tulo)\u001b[39;00m\n\u001b[32m     53\u001b[39m         loss_unsup = loss_obj(yhat_mask, logits_s_mask)        \u001b[38;5;66;03m# scalar\u001b[39;00m\n\u001b[32m     54\u001b[39m         loss = lambda_u * loss_unsup\n\u001b[32m     55\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     grads = tape.gradient(loss, model.trainable_variables)\n\u001b[32m     57\u001b[39m \n\u001b[32m     58\u001b[39m     \u001b[38;5;66;03m# Filtra gradientes None\u001b[39;00m\n\u001b[32m     59\u001b[39m     grads_vars = [(g, v) \u001b[38;5;28;01mfor\u001b[39;00m g, v \u001b[38;5;28;01min\u001b[39;00m zip(grads, model.trainable_variables) \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[32mc:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\ActiveSemisupervisedLearningCNN-s\\tf_env\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[39m\n\u001b[32m   1062\u001b[39m               output_gradients))\n\u001b[32m   1063\u001b[39m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[32m   1064\u001b[39m                           \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;28;01min\u001b[39;00m output_gradients]\n\u001b[32m   1065\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m     flat_grad = imperative_grad.imperative_grad(\n\u001b[32m   1067\u001b[39m         self._tape,\n\u001b[32m   1068\u001b[39m         flat_targets,\n\u001b[32m   1069\u001b[39m         flat_sources,\n",
      "\u001b[32mc:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\ActiveSemisupervisedLearningCNN-s\\tf_env\\Lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[39m\n\u001b[32m     63\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m ValueError:\n\u001b[32m     64\u001b[39m     raise ValueError(\n\u001b[32m     65\u001b[39m         \u001b[33m\"Unknown value for unconnected_gradients: %r\"\u001b[39m % unconnected_gradients)\n\u001b[32m     66\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[32m     68\u001b[39m       tape._tape,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m     69\u001b[39m       target,\n\u001b[32m     70\u001b[39m       sources,\n",
      "\u001b[32mc:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\ActiveSemisupervisedLearningCNN-s\\tf_env\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[39m\n\u001b[32m    144\u001b[39m     gradient_name_scope = \u001b[33m\"gradient_tape/\"\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m forward_pass_name_scope:\n\u001b[32m    146\u001b[39m       gradient_name_scope += forward_pass_name_scope + \u001b[33m\"/\"\u001b[39m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ops.name_scope(gradient_name_scope):\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, *out_grads)\n\u001b[32m    149\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, *out_grads)\n",
      "\u001b[32mc:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\ActiveSemisupervisedLearningCNN-s\\tf_env\\Lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(op, grad)\u001b[39m\n\u001b[32m    590\u001b[39m           padding=padding,\n\u001b[32m    591\u001b[39m           explicit_paddings=explicit_paddings,\n\u001b[32m    592\u001b[39m           use_cudnn_on_gpu=use_cudnn_on_gpu,\n\u001b[32m    593\u001b[39m           data_format=data_format),\n\u001b[32m--> \u001b[39m\u001b[32m594\u001b[39m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[32m    595\u001b[39m           op.inputs[\u001b[32m0\u001b[39m],\n\u001b[32m    596\u001b[39m           shape_1,\n\u001b[32m    597\u001b[39m           grad,\n",
      "\u001b[32mc:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\ActiveSemisupervisedLearningCNN-s\\tf_env\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[39m\n\u001b[32m   1495\u001b[39m         data_format, \u001b[33m\"dilations\"\u001b[39m, dilations)\n\u001b[32m   1496\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m   1497\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1498\u001b[39m       _ops.raise_from_not_ok_status(e, name)\n\u001b[32m-> \u001b[39m\u001b[32m1499\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._FallbackException:\n\u001b[32m   1500\u001b[39m       \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1501\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1502\u001b[39m       return conv2d_backprop_filter_eager_fallback(\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def acquire_topK_balanced(U_records, K, class_names):\n",
    "    import time\n",
    "    start = time.time()\n",
    "\n",
    "    per_class = max(1, K // max(1, len(class_names)))\n",
    "    buckets = {name: [] for name in class_names}\n",
    "\n",
    "    for rec in U_records:\n",
    "        sid = rec[\"sid\"]\n",
    "        p = rec[\"path\"].replace(\"\\\\\",\"/\").lower()\n",
    "        cname = None\n",
    "        for name in class_names:\n",
    "            if f\"/{name.lower()}/\" in p:\n",
    "                cname = name; break\n",
    "        if cname is None:\n",
    "            continue\n",
    "        mu_u, vu_u = U_MEAN.get(sid, 0.0), U_VAR.get(sid, 0.0)\n",
    "        mu_i, vi_i = I_MEAN.get(sid, 0.0), I_VAR.get(sid, 0.0)\n",
    "        score = ucb(mu_u, vu_u, c_u) * ucb(mu_i, vi_i, c_i)\n",
    "        buckets[cname].append((score, sid))\n",
    "\n",
    "    selected = []\n",
    "    for cname in class_names:\n",
    "        cand = sorted(buckets[cname], key=lambda x: x[0], reverse=True)\n",
    "        selected.extend([sid for _, sid in cand[:per_class]])\n",
    "\n",
    "    if len(selected) < K:\n",
    "        rest = []\n",
    "        have = set(selected)\n",
    "        for rec in U_records:\n",
    "            sid = rec[\"sid\"]\n",
    "            if sid in have: \n",
    "                continue\n",
    "            mu_u, vu_u = U_MEAN.get(sid, 0.0), U_VAR.get(sid, 0.0)\n",
    "            mu_i, vi_i = I_MEAN.get(sid, 0.0), I_VAR.get(sid, 0.0)\n",
    "            rest.append((ucb(mu_u,vu_u,c_u)*ucb(mu_i,vi_i,c_i), sid))\n",
    "        rest.sort(key=lambda x: x[0], reverse=True)\n",
    "        selected.extend([sid for _, sid in rest[:K-len(selected)]])\n",
    "\n",
    "    print(f\"acquire_topK_balanced: elapsed {time.time()-start:.4f} s\")\n",
    "    return selected[:K]\n",
    "\n",
    "def simulate_sid2true_from_path(U_records, L_records, class_map=CLASS_MAP):\n",
    "    sid2true = {}\n",
    "    def infer(path):\n",
    "        p = path.replace(\"\\\\\",\"/\").lower()\n",
    "        for name, idx in class_map.items():\n",
    "            if f\"/{name.lower()}/\" in p:\n",
    "                return idx\n",
    "        raise ValueError(f\"N√£o infere classe de: {path}\")\n",
    "    for rec in U_records: sid2true[rec[\"sid\"]] = infer(rec[\"path\"])\n",
    "    for rec in L_records: sid2true[rec[\"sid\"]] = infer(rec[\"path\"])\n",
    "    return sid2true\n",
    "\n",
    "\n",
    "def move_annotated_to_L(sid2label, U_records, L_records):\n",
    "    by_id = {r[\"sid\"]: r for r in U_records}\n",
    "    moved = []\n",
    "    keepU = []\n",
    "    for r in U_records:\n",
    "        sid = r[\"sid\"]\n",
    "        if sid in sid2label:\n",
    "            nr = dict(r)\n",
    "            nr[\"label\"] = int(sid2label[sid])\n",
    "            moved.append(nr)\n",
    "        else:\n",
    "            keepU.append(r)\n",
    "    return keepU, (L_records + moved)\n",
    "\n",
    "# ==== Dataset base ====\n",
    "train_root = r\"..\\data\\train\"\n",
    "val_root   = r\"..\\data\\validation\"\n",
    "\n",
    "all_train = build_records_from_dir(train_root, class_map=CLASS_MAP)\n",
    "L_records, U_records = split_L_U(all_train, n_L=400, seed=42)\n",
    "\n",
    "val_records = build_records_from_dir(val_root, class_map=CLASS_MAP)\n",
    "val_ds = make_val_ds(val_records, batch=BATCH, image_size=IMAGE_SIZE)\n",
    "\n",
    "# L_ds = make_L_ds(L_records, batch=BATCH, image_size=IMAGE_SIZE)\n",
    "# U_ds = make_U_ds(U_records, batch=BATCH, image_size=IMAGE_SIZE)\n",
    "\n",
    "CLASS_NAMES = list(CLASS_MAP.keys())  # ['Cat','Dog'] etc.\n",
    "weak_aug   = make_weak_aug()\n",
    "strong_aug = make_strong_aug(mild=True)\n",
    "\n",
    "L_ds = make_L_ds(L_records, batch=BATCH, image_size=IMAGE_SIZE)\n",
    "U_ds = make_U_ds(U_records, batch=BATCH, image_size=IMAGE_SIZE,\n",
    "                 weak_aug=weak_aug, strong_aug=strong_aug)  \n",
    "\n",
    "# Checar L\n",
    "xb, yb, _ = next(iter(L_ds))\n",
    "print(\"L min/max:\", float(tf.reduce_min(xb)), float(tf.reduce_max(xb)))\n",
    "\n",
    "# Checar U (weak/strong)\n",
    "xw, xs, _ = next(iter(U_ds))\n",
    "print(\"U weak min/max:\", float(tf.reduce_min(xw)), float(tf.reduce_max(xw)))\n",
    "print(\"U strong min/max:\", float(tf.reduce_min(xs)), float(tf.reduce_max(xs)))\n",
    "\n",
    "\n",
    "# simula√ß√£o de anota√ß√£o (sem arquivos)\n",
    "sid2true = simulate_sid2true_from_path(U_records, L_records, class_map=CLASS_MAP)\n",
    "\n",
    "ROUNDS = 10\n",
    "EPOCHS_PER_ROUND = 5\n",
    "\n",
    "for r in range(ROUNDS):\n",
    "    print(f\"== Rodada {r+1}/{ROUNDS} ==\")\n",
    "\n",
    "    # --- primeiras √©pocas da rodada: mild ---\n",
    "    strong_aug = make_strong_aug(mild=True)\n",
    "    U_ds = make_U_ds(U_records, batch=BATCH, image_size=IMAGE_SIZE,\n",
    "                     weak_aug=weak_aug, strong_aug=strong_aug)\n",
    "\n",
    "    run_one_round(L_ds, U_ds, epoch_idx=r, epochs=2, lambda_u_max=1.0, temp_T=0.5)\n",
    "\n",
    "    # --- depois, for√ßa total ---\n",
    "    strong_aug = make_strong_aug(mild=False)\n",
    "    U_ds = make_U_ds(U_records, batch=BATCH, image_size=IMAGE_SIZE,\n",
    "                     weak_aug=weak_aug, strong_aug=strong_aug)\n",
    "\n",
    "    run_one_round(L_ds, U_ds, epoch_idx=r, epochs=EPOCHS_PER_ROUND-2, lambda_u_max=1.0, temp_T=0.5)\n",
    "\n",
    "    # avalia√ß√£o\n",
    "    vloss, vacc = evaluate(model, val_ds)\n",
    "    print(f\"Val loss: {vloss:.4f}  Val acc: {vacc:.4f}\")\n",
    "\n",
    "    # sele√ß√£o ativa balanceada\n",
    "    selected_ids = acquire_topK_balanced(U_records, K=40, class_names=CLASS_NAMES) # verificar qual a melhor qtd de amostra ideal\n",
    "\n",
    "    # ‚Äúanota√ß√£o‚Äù simulada apenas nos K escolhidos\n",
    "    sid2label = {sid: sid2true[sid] for sid in selected_ids}\n",
    "    # mover U->L\n",
    "    U_records, L_records = move_annotated_to_L(sid2label, U_records, L_records)\n",
    "\n",
    "    # limpar EMA/UCB dos sids movidos\n",
    "    moved_set = set(sid2label.keys())\n",
    "    for sid in list(U_MEAN.keys()):\n",
    "        if sid in moved_set: U_MEAN.pop(sid, None); U_VAR.pop(sid, None)\n",
    "    for sid in list(I_MEAN.keys()):\n",
    "        if sid in moved_set: I_MEAN.pop(sid, None); I_VAR.pop(sid, None)\n",
    "\n",
    "    # rebuild datasets\n",
    "    L_ds = make_L_ds(L_records, batch=BATCH, image_size=IMAGE_SIZE)\n",
    "    U_ds = make_U_ds(U_records, batch=BATCH, image_size=IMAGE_SIZE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
