{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "266bfc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, json, math, random, pathlib\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Performance: enable mixed precision when available (speeds up training on modern GPUs)\n",
    "# try:\n",
    "#     from tensorflow.keras import mixed_precision\n",
    "#     mixed_precision.set_global_policy('mixed_float16')\n",
    "#     print(\"Mixed precision enabled: float16 compute, float32 variables\")\n",
    "# except Exception as _e:\n",
    "#     # fallback: mixed precision not available on this platform / TF build\n",
    "#     print(\"Mixed precision not enabled (platform TF build may not support it):\", _e)\n",
    "\n",
    "# ==== CONFIG ====\n",
    "H, W = 180, 180\n",
    "IMAGE_SIZE = (H, W)\n",
    "# Reduced BATCH for quick verification (was 64)\n",
    "BATCH = 16\n",
    "NUM_CLASSES = 2\n",
    "CLASS_NAMES = (\"Cat\", \"Dog\")   # nomes como estÃ£o nas pastas\n",
    "CLASS_MAP = {name: i for i, name in enumerate(CLASS_NAMES)}\n",
    "INV_CLASS_MAP = {v:k for k,v in CLASS_MAP.items()}\n",
    "# Limit unlabeled steps per epoch during quick checks (was 500)\n",
    "MAX_U_STEPS = 50 # small value for fast verification; None = use all\n",
    "DEBUG = True                  # True = logs verbosos, sem @tf.function no passo U\n",
    "SHUFFLE_BUF_L = 1024          # buffers menores evitam longas â€œfilling upâ€¦â€\n",
    "SHUFFLE_BUF_U = 1024\n",
    "PREFETCH = tf.data.AUTOTUNE\n",
    "LOG_EVERY_L = 50              # log a cada N batches L\n",
    "LOG_EVERY_U = 50  \n",
    "\n",
    "tau = 0.7          # threshold inicial mais brando\n",
    "lambda_u_max = 1.0 # peso mÃ¡ximo da perda unsupervisionada\n",
    "c_u, c_i = 1.0, 1.0  # coef. UCB\n",
    "alpha = 0.9          # EMA\n",
    "\n",
    "# Threading to avoid CPU oversubscription\n",
    "# try:\n",
    "#     tf.config.threading.set_inter_op_parallelism_threads(0)\n",
    "#     tf.config.threading.set_intra_op_parallelism_threads(0)\n",
    "# except Exception:\n",
    "#     pass\n",
    "\n",
    "# # Auto-adjust batch if GPU present (simple heuristic)\n",
    "# _gpus = tf.config.list_physical_devices('GPU')\n",
    "# if _gpus:\n",
    "#     # try to increase batch multiplicatively to improve throughput\n",
    "#     old_batch = BATCH\n",
    "#     BATCH = int(BATCH * max(1, len(_gpus) * 2))\n",
    "#     print(f\"Detected {_gpus and len(_gpus)} GPU(s); increasing BATCH {old_batch} -> {BATCH}\")\n",
    "# else:\n",
    "#     print(\"No GPUs detected; keeping default BATCH\")\n",
    "\n",
    "# # Cache decision will be computed after dataset listing (we may use psutil if available)\n",
    "# CACHE_DATA = None  # None = decide automatically later, True/False forced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3da6b4",
   "metadata": {},
   "source": [
    "### Helpers de leitura e record builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f6bc9065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_decode_resize_jpeg(path, image_size):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3, dct_method='INTEGER_ACCURATE')\n",
    "    img = tf.image.resize(img, image_size, antialias=True)\n",
    "    img = tf.cast(img, tf.float32) / 255.0  # [0,1] â€” ÃšNICA normalizaÃ§Ã£o\n",
    "    return img\n",
    "\n",
    "IGNORE_ERRORS = tf.data.experimental.ignore_errors()\n",
    "\n",
    "def build_records_from_dir(root_dir, class_map=CLASS_MAP, with_ids=True, seed=42):\n",
    "    \"\"\"\n",
    "    Varre .../Cat/*.jpg e .../Dog/*.jpg gerando records {sid, path, label}\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    records = []\n",
    "    sid = 0\n",
    "    for name, idx in class_map.items():\n",
    "        cls_dir = os.path.join(root_dir, name)\n",
    "        for fn in os.listdir(cls_dir):\n",
    "            if not fn.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                continue\n",
    "            p = os.path.join(cls_dir, fn)\n",
    "            rec = {\"sid\": sid, \"path\": p}\n",
    "            if class_map is not None:\n",
    "                rec[\"label\"] = idx\n",
    "            records.append(rec)\n",
    "            sid += 1\n",
    "    rng.shuffle(records)\n",
    "    return records\n",
    "\n",
    "def split_L_U(records, n_L=400, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    arr = records.copy()\n",
    "    rng.shuffle(arr)\n",
    "    L = []\n",
    "    U = []\n",
    "    for r in arr:\n",
    "        if len(L) < n_L:\n",
    "            L.append(r)\n",
    "        else:\n",
    "            U.append(r)\n",
    "    return L, U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4df7e6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_decode_resize_jpeg(path, image_size):\n",
    "    # path Ã© tf.Tensor (scalar string)\n",
    "    img_bytes = tf.io.read_file(path)\n",
    "\n",
    "    # decode robusto a JPEG com headers estranhos\n",
    "    img = tf.image.decode_jpeg(img_bytes, channels=3, dct_method='INTEGER_ACCURATE')\n",
    "\n",
    "    # resize com antialias\n",
    "    img = tf.image.resize(img, image_size, antialias=True)\n",
    "\n",
    "    # cast e normalizaÃ§Ã£o AQUI (opÃ§Ã£o B)\n",
    "    img = tf.cast(img, tf.float32) / 255.0  # <- garante [0,1]\n",
    "\n",
    "    # (opcional) clip de seguranÃ§a por ruÃ­do/artefatos\n",
    "    img = tf.clip_by_value(img, 0.0, 1.0)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832d6a60",
   "metadata": {},
   "source": [
    "### Augmentations batch-first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "943cd0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_aug_seq = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomTranslation(0.05, 0.05),\n",
    "], name=\"weak_aug\")\n",
    "\n",
    "strong_aug_seq = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.15),\n",
    "    layers.RandomZoom(0.15),\n",
    "    layers.RandomContrast(0.15),\n",
    "], name=\"strong_aug\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fd06804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== AUGMENTS ================================================================\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "IMAGE_SIZE = (180, 180)  # ajuste ao seu valor\n",
    "\n",
    "def make_weak_aug():\n",
    "    # Flip + pequeno crop â†’ mantÃ©m semÃ¢ntica (FixMatch \"weak\")\n",
    "    return keras.Sequential([\n",
    "        # keras.layers.RandomFlip(\"horizontal\"),\n",
    "        # keras.layers.RandomTranslation(height_factor=0.02, width_factor=0.02, fill_mode=\"reflect\"),\n",
    "        keras.layers.RandomFlip(\"horizontal\"),\n",
    "        keras.layers.RandomRotation(0.05),\n",
    "        keras.layers.RandomTranslation(height_factor=0.02, width_factor=0.02, fill_mode=\"reflect\"),\n",
    "        layers.Lambda(lambda x: tf.clip_by_value(x, 0.0, 1.0)),\n",
    "    ])\n",
    "\n",
    "def make_strong_aug(mild=False):\n",
    "    if mild:\n",
    "        # primeiras Ã©pocas: menos agressivo\n",
    "        return keras.Sequential([\n",
    "            keras.layers.RandomFlip(\"horizontal\"),\n",
    "            keras.layers.RandomRotation(0.15),\n",
    "            keras.layers.RandomContrast(0.2),\n",
    "            keras.layers.Lambda(lambda x: tf.clip_by_value(x, 0.0, 1.0)),\n",
    "        ])\n",
    "    # depois: mais agressivo\n",
    "    return keras.Sequential([\n",
    "        keras.layers.RandomFlip(\"horizontal\"),\n",
    "        keras.layers.RandomRotation(0.25),\n",
    "        keras.layers.RandomContrast(0.4),\n",
    "        keras.layers.RandomBrightness(factor=0.2),\n",
    "        keras.layers.Lambda(lambda x: tf.clip_by_value(x, 0.0, 1.0)),\n",
    "    ])\n",
    "\n",
    "weak_aug   = make_weak_aug()\n",
    "strong_aug = make_strong_aug(mild=True)   # comeÃ§amos \"mild\"; trocaremos no loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ebf26d",
   "metadata": {},
   "source": [
    "### Datasets (versÃµes Ãºnicas, sem duplicidade de batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e25a4f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_ERRORS = tf.data.experimental.ignore_errors()\n",
    "\n",
    "def make_L_ds(L_records, batch, image_size, drop_remainder=False, cache=False):\n",
    "    paths  = [r[\"path\"] for r in L_records]\n",
    "    labels = [r[\"label\"] for r in L_records]\n",
    "    ids    = [r[\"sid\"]   for r in L_records]\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels, ids))\n",
    "\n",
    "    def _load(path, y, sid):\n",
    "        img = _safe_decode_resize_jpeg(path, image_size)  # [0,1]\n",
    "        return img, y, sid\n",
    "\n",
    "    ds = ds.shuffle(SHUFFLE_BUF_L, reshuffle_each_iteration=True)\n",
    "    ds = ds.map(_load, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    ds = ds.apply(IGNORE_ERRORS)\n",
    "\n",
    "    # if cache:\n",
    "    #     try:\n",
    "    #         ds = ds.cache()\n",
    "    #         print(f\"L_ds: caching enabled (may use memory/disk)\")\n",
    "    #     except Exception as _e:\n",
    "    #         print(\"L_ds: cache() failed:\", _e)\n",
    "\n",
    "    ds = ds.batch(batch, drop_remainder=drop_remainder)\n",
    "    ds = ds.prefetch(PREFETCH)\n",
    "    return ds\n",
    "\n",
    "def make_U_ds(U_records, batch, image_size, drop_remainder=False,\n",
    "              weak_aug=None, strong_aug=None, cache=False):\n",
    "    paths = [r[\"path\"] for r in U_records]\n",
    "    ids   = [r[\"sid\"]  for r in U_records]\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, ids))\n",
    "\n",
    "    def _loadU(path, sid):\n",
    "        img = _safe_decode_resize_jpeg(path, image_size)  # [0,1]\n",
    "        return img, sid\n",
    "\n",
    "    ds = ds.shuffle(SHUFFLE_BUF_U, reshuffle_each_iteration=True)\n",
    "    ds = ds.map(_loadU, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    ds = ds.apply(IGNORE_ERRORS)\n",
    "\n",
    "    # if cache:\n",
    "    #     try:\n",
    "    #         ds = ds.cache()\n",
    "    #         print(f\"U_ds: caching enabled (may use memory/disk)\")\n",
    "    #     except Exception as _e:\n",
    "    #         print(\"U_ds: cache() failed:\", _e)\n",
    "\n",
    "    ds = ds.batch(batch, drop_remainder=drop_remainder)\n",
    "\n",
    "    # aplica augment APÃ“S o batch\n",
    "    def _make_pair(batch_img, batch_sid):\n",
    "        # prefer aug functions/seq passados como argumento; fallback para globals\n",
    "        weak = weak_aug if weak_aug is not None else weak_aug_seq\n",
    "        strong = strong_aug if strong_aug is not None else strong_aug_seq\n",
    "\n",
    "        x_w = weak(batch_img, training=True)\n",
    "        x_s = strong(batch_img, training=True)\n",
    "        # redundante se jÃ¡ hÃ¡ Lambda nas seqs, mas seguro:\n",
    "        x_w = tf.clip_by_value(x_w, 0.0, 1.0)\n",
    "        x_s = tf.clip_by_value(x_s, 0.0, 1.0)\n",
    "        return x_w, x_s, batch_sid\n",
    "\n",
    "    ds = ds.map(_make_pair, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    ds = ds.prefetch(PREFETCH)\n",
    "    return ds\n",
    "\n",
    "def make_val_ds(val_records, batch, image_size):\n",
    "    paths  = [r[\"path\"] for r in val_records]\n",
    "    labels = [r[\"label\"] for r in val_records]\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "\n",
    "    def _load(path, y):\n",
    "        img = _safe_decode_resize_jpeg(path, image_size)  # [0,1]\n",
    "        return img, y\n",
    "\n",
    "    ds = ds.map(_load, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c180f",
   "metadata": {},
   "source": [
    "### Modelo Ãºnico com normalizaÃ§Ã£o no modelo (evita duplicidade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7f9f825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes=NUM_CLASSES, image_size=IMAGE_SIZE):\n",
    "    inp = keras.Input(shape=image_size + (3,))\n",
    "    x = layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    out = layers.Dense(num_classes)(x)  # logits\n",
    "    # ensure output is float32 even under mixed precision\n",
    "    # out = layers.Activation('linear', dtype='float32')(out)\n",
    "    return keras.Model(inp, out)\n",
    "\n",
    "model = build_model()\n",
    "# use optimizer with loss scaling if mixed precision is enabled\n",
    "# try:\n",
    "#     from tensorflow.keras import mixed_precision\n",
    "#     if mixed_precision.global_policy().name == 'mixed_float16':\n",
    "#         optimizer = mixed_precision.LossScaleOptimizer(keras.optimizers.Adam(1e-3))\n",
    "#     else:\n",
    "#         optimizer = keras.optimizers.Adam(1e-3)\n",
    "# except Exception:\n",
    "#     optimizer = keras.optimizers.Adam(1e-3)\n",
    "optimizer = keras.optimizers.Adam(1e-3)\n",
    "\n",
    "loss_obj  = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d0811a",
   "metadata": {},
   "source": [
    "### MÃ©tricas por amostra, EMA/UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6535e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_logits(logits):\n",
    "    return tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "def pseudo_el2n(probs):  # incerteza (EL2N-like, proxy simples)\n",
    "    # distÃ¢ncia do one-hot do argmax\n",
    "    yhat = tf.argmax(probs, axis=-1)\n",
    "    oh = tf.one_hot(yhat, depth=tf.shape(probs)[-1])\n",
    "    return tf.reduce_mean(tf.square(probs - oh), axis=-1)\n",
    "\n",
    "def sym_kl(p, q, eps=1e-7):\n",
    "    p = tf.clip_by_value(p, eps, 1.0)\n",
    "    q = tf.clip_by_value(q, eps, 1.0)\n",
    "    kl1 = tf.reduce_sum(p * tf.math.log(p/q), axis=-1)\n",
    "    kl2 = tf.reduce_sum(q * tf.math.log(q/p), axis=-1)\n",
    "    return 0.5*(kl1+kl2)\n",
    "\n",
    "# tabelas EMA (legacy dicts kept for compatibility)\n",
    "u_mean, u_var = {}, {}\n",
    "i_mean, i_var = {}, {}\n",
    "\n",
    "# tabelas EMA (GLOBAIS): arrays will be initialized after dataset listing for vectorized updates\n",
    "U_MEAN, U_VAR = {}, {}\n",
    "I_MEAN, I_VAR = {}, {}\n",
    "U_MEAN_ARR = None\n",
    "U_VAR_ARR = None\n",
    "I_MEAN_ARR = None\n",
    "I_VAR_ARR = None\n",
    "\n",
    "def initialize_ema_arrays(n):\n",
    "    \"\"\"Create numpy arrays (float64) of length n for EMA/UCB statistics.\"\"\"\n",
    "    global U_MEAN_ARR, U_VAR_ARR, I_MEAN_ARR, I_VAR_ARR\n",
    "    U_MEAN_ARR = np.zeros(n, dtype=np.float64)\n",
    "    U_VAR_ARR  = np.zeros(n, dtype=np.float64)\n",
    "    I_MEAN_ARR = np.zeros(n, dtype=np.float64)\n",
    "    I_VAR_ARR  = np.zeros(n, dtype=np.float64)\n",
    "    print(f\"Initialized EMA arrays length={n}\")\n",
    "\n",
    "def ema_update_batch(mean_arr, var_arr, sids, vals, alpha=0.9):\n",
    "    \"\"\"Vectorized EMA update for arrays. sids and vals are numpy arrays or tensors.\"\"\"\n",
    "    if mean_arr is None or var_arr is None:\n",
    "        return\n",
    "    s = np.asarray(sids, dtype=np.int64)\n",
    "    v = np.asarray(vals, dtype=np.float64)\n",
    "    m = mean_arr[s]\n",
    "    vv = var_arr[s]\n",
    "    new_m = alpha*m + (1.0-alpha)*v\n",
    "    new_v = alpha*vv + (1.0-alpha)*(v - new_m)**2\n",
    "    mean_arr[s] = new_m\n",
    "    var_arr[s] = new_v\n",
    "\n",
    "def ema_update(mean_dict, var_dict, sid, val, alpha=0.9):\n",
    "    # fallback scalar update (kept for compatibility)\n",
    "    sid = int(sid)\n",
    "    val = float(val)\n",
    "    m = mean_dict.get(sid, val)\n",
    "    v = var_dict.get(sid, 0.0)\n",
    "    new_m = alpha*m + (1.0-alpha)*val\n",
    "    new_v = alpha*v + (1.0-alpha)*(val - new_m)**2\n",
    "    mean_dict[sid] = float(new_m)\n",
    "    var_dict[sid]  = float(new_v)\n",
    "\n",
    "def ucb(mu, var, c=1.0):\n",
    "    return mu + c * math.sqrt(max(var, 1e-12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "59413b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probe_confidences_on_U(model, U_ds, temp_T=0.5, \n",
    "                           max_batches=8,  # smaller probe for quick verification (was 32)\n",
    "                           stop_if_n=2048   # stop earlier when enough confs collected\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Faz forward em alguns batches de U para medir confianÃ§as da WEAK.\n",
    "    Retorna uma lista de confianÃ§as (max softmax).\n",
    "    \"\"\"\n",
    "    confs = []\n",
    "    n_batches = 0\n",
    "    for x_w, _, _ in U_ds:  # U_ds -> (x_w, x_s, sid)\n",
    "        logits_w = model(x_w, training=False)\n",
    "        probs_w  = tf.nn.softmax(logits_w / temp_T, axis=-1)\n",
    "        conf     = tf.reduce_max(probs_w, axis=-1).numpy()  # (B,)\n",
    "        confs.append(conf)\n",
    "        n_batches += 1\n",
    "        if n_batches >= max_batches or sum(len(c) for c in confs) >= stop_if_n:\n",
    "            break\n",
    "    if len(confs) == 0:\n",
    "        return []\n",
    "    return np.concatenate(confs, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7668c7",
   "metadata": {},
   "source": [
    "### Passos de treino (supervisionado e nÃ£o-rotulado) â€” sem dupla normalizaÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "71c2f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_supervised(batch):\n",
    "    x_l, y_l, _ = batch\n",
    "    start = time.time() if DEBUG else None\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits_l = model(x_l, training=True)  # Rescaling acontece no modelo\n",
    "        loss_sup = loss_obj(y_l, logits_l)\n",
    "\n",
    "    grads = tape.gradient(loss_sup, model.trainable_variables)\n",
    "\n",
    "    # If optimizer supports unscaling (LossScaleOptimizer), use it\n",
    "    if hasattr(optimizer, 'get_unscaled_gradients'):\n",
    "        try:\n",
    "            grads = optimizer.get_unscaled_gradients(grads)\n",
    "        except Exception:\n",
    "            # some TF versions expose a different API; keep grads as-is\n",
    "            pass\n",
    "\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    if DEBUG and start is not None:\n",
    "        print(f\"train_step_supervised: elapsed {time.time()-start:.4f} s\")\n",
    "\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(logits_l, -1), tf.cast(y_l, tf.int64)), tf.float32)\n",
    "    )\n",
    "    return loss_sup, acc\n",
    "\n",
    "\n",
    "def train_step_unlabeled_with_masklog(batch, lambda_u=1.0, tau=0.7, temp_T=0.5):\n",
    "    x_w, x_s, sids = batch\n",
    "    start_total = time.time() if DEBUG else None\n",
    "\n",
    "    # ---------- forward (fora do tape) para mÃ¡scara e mÃ©tricas ----------\n",
    "    t0 = time.time() if DEBUG else None\n",
    "    logits_w = model(x_w, training=True)\n",
    "    probs_w  = tf.nn.softmax(logits_w / temp_T, axis=-1)\n",
    "    if DEBUG and t0 is not None:\n",
    "        print(f\"unlabeled: forward weak took {time.time()-t0:.4f} s\")\n",
    "\n",
    "    # mÃ©tricas (incerteza U, inconsistÃªncia I) e mÃ¡scara\n",
    "    conf = tf.reduce_max(probs_w, axis=-1)           # [B]\n",
    "    yhat = tf.argmax(probs_w, axis=-1)               # [B], int\n",
    "    mask = conf >= tau                               # [B], bool\n",
    "    masked = tf.where(mask)                          # [M,1]\n",
    "    masked_count = tf.shape(masked)[0]\n",
    "\n",
    "    # ConsistÃªncia com strong (para logging apenas â€“ fora do tape)\n",
    "    t1 = time.time() if DEBUG else None\n",
    "    logits_s_for_I = model(x_s, training=True)\n",
    "    probs_s_for_I  = tf.nn.softmax(logits_s_for_I, axis=-1)\n",
    "    if DEBUG and t1 is not None:\n",
    "        print(f\"unlabeled: forward strong(for I) took {time.time()-t1:.4f} s\")\n",
    "\n",
    "    u_batch = pseudo_el2n(probs_w)                   # [B]\n",
    "    i_batch = sym_kl(probs_w, probs_s_for_I)         # [B]\n",
    "\n",
    "    # Sem exemplos vÃ¡lidos ou sem peso â†’ nÃ£o atualiza\n",
    "    if (lambda_u <= 0.0) or tf.equal(masked_count, 0):\n",
    "        if DEBUG and start_total is not None:\n",
    "            print(f\"unlabeled: skipped (no masked) elapsed {time.time()-start_total:.4f} s\")\n",
    "        return {\n",
    "            \"loss\": tf.constant(0.0, tf.float32),\n",
    "            \"u_batch\": u_batch,\n",
    "            \"i_batch\": i_batch,\n",
    "            \"sids\": sids,\n",
    "            \"mask_pass\": tf.reduce_sum(tf.cast(mask, tf.int32)),\n",
    "            \"mask_total\": tf.size(mask),\n",
    "            \"skipped\": True,\n",
    "        }\n",
    "\n",
    "    # ---------- forward DENTRO do tape (grava computaÃ§Ã£o para gradiente) ----------\n",
    "    t2 = time.time() if DEBUG else None\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits_s = model(x_s, training=True)                   # recomputa dentro do tape\n",
    "        logits_s_mask = tf.gather(logits_s, masked[:, 0])      # [M, C]\n",
    "        yhat_mask     = tf.gather(yhat,     masked[:, 0])      # [M]\n",
    "        # perda de consistÃªncia (pseudo-rÃ³tulo)\n",
    "        loss_unsup = loss_obj(yhat_mask, logits_s_mask)        # scalar\n",
    "        loss = lambda_u * loss_unsup\n",
    "\n",
    "        # handle mixed-precision loss scaling if optimizer provides it\n",
    "        if hasattr(optimizer, 'get_scaled_loss'):\n",
    "            try:\n",
    "                scaled_loss = optimizer.get_scaled_loss(loss)\n",
    "            except Exception:\n",
    "                scaled_loss = loss\n",
    "        else:\n",
    "            scaled_loss = loss\n",
    "\n",
    "    if DEBUG and t2 is not None:\n",
    "        print(f\"unlabeled: forward+loss(in tape) took {time.time()-t2:.4f} s\")\n",
    "\n",
    "    grads = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "\n",
    "    # If optimizer supports unscaling (LossScaleOptimizer), use it\n",
    "    if hasattr(optimizer, 'get_unscaled_gradients'):\n",
    "        try:\n",
    "            grads = optimizer.get_unscaled_gradients(grads)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Filtra gradientes None\n",
    "    grads_vars = [(g, v) for g, v in zip(grads, model.trainable_variables) if g is not None]\n",
    "    if not grads_vars:\n",
    "        if DEBUG and start_total is not None:\n",
    "            print(f\"unlabeled: no grads (skipped) elapsed {time.time()-start_total:.4f} s\")\n",
    "        return {\n",
    "            \"loss\": tf.constant(0.0, tf.float32),\n",
    "            \"u_batch\": u_batch,\n",
    "            \"i_batch\": i_batch,\n",
    "            \"sids\": sids,\n",
    "            \"mask_pass\": tf.reduce_sum(tf.cast(mask, tf.int32)),\n",
    "            \"mask_total\": tf.size(mask),\n",
    "            \"skipped\": True,\n",
    "        }\n",
    "\n",
    "    t3 = time.time() if DEBUG else None\n",
    "    optimizer.apply_gradients(grads_vars)\n",
    "    if DEBUG and t3 is not None and start_total is not None:\n",
    "        print(f\"unlabeled: apply_gradients took {time.time()-t3:.4f} s  total elapsed {time.time()-start_total:.4f} s\")\n",
    "\n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"u_batch\": u_batch,\n",
    "        \"i_batch\": i_batch,\n",
    "        \"sids\": sids,\n",
    "        \"mask_pass\": tf.reduce_sum(tf.cast(mask, tf.int32)),\n",
    "        \"mask_total\": tf.size(mask),\n",
    "        \"skipped\": False,\n",
    "    }\n",
    "\n",
    "# Conditionally compile the train steps for speed when not in DEBUG\n",
    "if not DEBUG:\n",
    "    train_step_supervised = tf.function(train_step_supervised)\n",
    "    train_step_unlabeled_with_masklog = tf.function(train_step_unlabeled_with_masklog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b85eb5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== SCHEDULES ==============================================================\n",
    "def tau_schedule(epoch):\n",
    "    # 0â†’0.6, 1â†’0.7, 2â†’0.8, saturando em 0.95 (se quiser, aumente depois)\n",
    "    return min(0.95, 0.6 + 0.1*epoch)\n",
    "\n",
    "def lambda_u_warmup(epoch, total=3, max_val=1.0):\n",
    "    # sobe rÃ¡pido para dar peso ao U cedo\n",
    "    return max_val * min(1.0, (epoch+1)/total)\n",
    "\n",
    "def choose_adaptive_tau(conf_list, target_rate=0.6, \n",
    "                        tau_min=0.40, tau_max=0.95,\n",
    "                        prev_tau=None, smooth=0.6):\n",
    "    \"\"\"\n",
    "    conf_list: lista/np.array de confianÃ§as (max softmax da weak)\n",
    "    target_rate: fraÃ§Ã£o de amostras que devem PASSAR na mÃ¡scara (ex.: 0.6 -> 60%)\n",
    "    tau_min/tau_max: limites de seguranÃ§a\n",
    "    prev_tau: tau da Ã©poca anterior (para suavizaÃ§Ã£o exponencial)\n",
    "    smooth: fator de suavizaÃ§Ã£o (0..1). 1.0 = sem suavizaÃ§Ã£o\n",
    "    \"\"\"\n",
    "    if len(conf_list) == 0:\n",
    "        # fallback conservador\n",
    "        base = 0.5\n",
    "        return float(prev_tau if prev_tau is not None else base)\n",
    "\n",
    "    # Queremos que ~target_rate passem: tau = quantil de (1 - target_rate)\n",
    "    q = float(np.clip(1.0 - target_rate, 0.0, 1.0))\n",
    "    raw_tau = float(np.quantile(np.asarray(conf_list), q))\n",
    "    raw_tau = float(np.clip(raw_tau, tau_min, tau_max))\n",
    "\n",
    "    if prev_tau is None:\n",
    "        return raw_tau\n",
    "    # suavizaÃ§Ã£o exponencial: mais estÃ¡vel entre Ã©pocas\n",
    "    return float(smooth * prev_tau + (1.0 - smooth) * raw_tau)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "87e2e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_round(L_ds, U_ds, epoch_idx, epochs=5, \n",
    "                  lambda_u_max=1.0, \n",
    "                  base_tau=0.60, \n",
    "                  temp_T=0.5,\n",
    "                  target_rate_mild=0.60,   # ~60% passam na fase mild\n",
    "                  target_rate_strong=0.50, # ~50% na fase forte\n",
    "                  is_strong_phase=False,    # defina True/False fora conforme sua agenda mildâ†’strong\n",
    "                  prev_tau=None):           # opcional: passa o tau da Ã©poca anterior p/ suavizar\n",
    "    \"\"\"\n",
    "    Retorna o Ãºltimo tau usado (para vocÃª reutilizar/suavizar na prÃ³xima chamada).\n",
    "    \"\"\"\n",
    "    tau_used = prev_tau if prev_tau is not None else base_tau\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        # ---- 1) Supervisionado (igual)\n",
    "        tL0 = time.time()\n",
    "        l_loss_sum, l_acc_sum, l_batches = 0.0, 0.0, 0\n",
    "        for batch in L_ds:\n",
    "            loss_sup, acc = train_step_supervised(batch)\n",
    "            l_loss_sum += float(loss_sup); l_acc_sum += float(acc); l_batches += 1\n",
    "        tL1 = time.time()\n",
    "\n",
    "        # ---- 2) Probe confianÃ§as da WEAK para escolher tau adaptativo desta Ã©poca\n",
    "        confs = probe_confidences_on_U(model, U_ds, temp_T=temp_T,\n",
    "                                       max_batches=8, stop_if_n=2048)\n",
    "        target_rate = (target_rate_strong if is_strong_phase else target_rate_mild)\n",
    "        tau_used = choose_adaptive_tau(confs, target_rate=target_rate, \n",
    "                                       tau_min=0.40, tau_max=0.95,\n",
    "                                       prev_tau=tau_used, smooth=0.6)\n",
    "\n",
    "        # ---- 3) NÃ£o-rotulado (FixMatch) com tau adaptativo\n",
    "        lam = lambda_u_warmup(ep, total=3, max_val=lambda_u_max)\n",
    "        tU0 = time.time()\n",
    "        u_batches, skipped_all = 0, 0\n",
    "        pass_sum, total_sum = 0, 0\n",
    "        u_mean_local, i_mean_local = 0.0, 0.0\n",
    "\n",
    "        for batch in U_ds:\n",
    "            out = train_step_unlabeled_with_masklog(\n",
    "                batch, lambda_u=lam, tau=tau_used, temp_T=temp_T\n",
    "            )\n",
    "            u_batches += 1\n",
    "            skipped_all += int(out[\"skipped\"])\n",
    "\n",
    "            # Respect debug short-run limit\n",
    "            if MAX_U_STEPS is not None and u_batches >= MAX_U_STEPS:\n",
    "                if DEBUG:\n",
    "                    print(f\"Reached MAX_U_STEPS={MAX_U_STEPS}; breaking U loop for debug\")\n",
    "                break\n",
    "\n",
    "            # mÃ©tricas\n",
    "            pass_sum  += int(out[\"mask_pass\"].numpy())\n",
    "            total_sum += int(out[\"mask_total\"].numpy())\n",
    "            u_mean_local += float(tf.reduce_mean(out[\"u_batch\"]))\n",
    "            i_mean_local += float(tf.reduce_mean(out[\"i_batch\"]))\n",
    "\n",
    "            # EMA/UCB: vectorized when arrays exist\n",
    "            u_vals  = out[\"u_batch\"].numpy()\n",
    "            i_vals  = out[\"i_batch\"].numpy()\n",
    "            sid_vals= out[\"sids\"].numpy()\n",
    "            if U_MEAN_ARR is not None:\n",
    "                # vectorized batch update\n",
    "                ema_update_batch(U_MEAN_ARR, U_VAR_ARR, sid_vals, u_vals, alpha=alpha)\n",
    "                ema_update_batch(I_MEAN_ARR, I_VAR_ARR, sid_vals, i_vals, alpha=alpha)\n",
    "            else:\n",
    "                # fallback scalar updates\n",
    "                for val_u, val_i, sid in zip(u_vals, i_vals, sid_vals):\n",
    "                    ema_update(U_MEAN, U_VAR, int(sid), float(val_u), alpha=0.9)\n",
    "                    ema_update(I_MEAN, I_VAR, int(sid), float(val_i), alpha=0.9)\n",
    "\n",
    "        tU1 = time.time()\n",
    "\n",
    "        mask_rate = (pass_sum / max(total_sum, 1)) * 100.0\n",
    "        print(f\"[epoch {epoch_idx}:{ep+1}/{epochs}]  \"\n",
    "              f\"tau={tau_used:.2f}  Î»_u={lam:.2f}  T={temp_T:.2f}  \"\n",
    "              f\"mask_rate={mask_rate:.1f}%  skipped_batches={skipped_all}/{u_batches}  \"\n",
    "              f\"L_loss~={l_loss_sum/max(l_batches,1):.4f}  L_acc~={l_acc_sum/max(l_batches,1):.3f}  \"\n",
    "              f\"U~={u_mean_local/max(u_batches,1):.4f}  I~={i_mean_local/max(u_batches,1):.4f}  \"\n",
    "              f\"({tL1-tL0:.1f}s L, {tU1-tU0:.1f}s U)\")\n",
    "\n",
    "    return tau_used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e9c674",
   "metadata": {},
   "source": [
    "### AvaliaÃ§Ã£o e execuÃ§Ã£o de uma rodada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "baa562cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_ds):\n",
    "    tot, ok, loss_sum, n_batches = 0, 0, 0.0, 0\n",
    "    for x, y in val_ds:\n",
    "        logits = model(x, training=False)\n",
    "        loss_sum += float(loss_obj(y, logits))\n",
    "        ok += int((tf.argmax(logits, -1).numpy() == y.numpy()).sum())\n",
    "        tot += int(y.shape[0])\n",
    "        n_batches += 1\n",
    "    mean_loss = loss_sum / max(n_batches, 1)\n",
    "    acc = ok / max(tot, 1)\n",
    "    return mean_loss, acc\n",
    "\n",
    "# ==== LOOP DE UMA RODADA =====================================================\n",
    "# def run_one_round(L_ds, U_ds, epoch_idx, epochs=5, lambda_u_max=1.0, temp_T=0.5):\n",
    "#     import time\n",
    "#     for ep in range(epochs):\n",
    "#         current_tau = tau_schedule(ep)\n",
    "#         lam = lambda_u_warmup(ep, total=3, max_val=lambda_u_max)\n",
    "\n",
    "#         # (Opcional) logging rÃ¡pido de confianÃ§a do 1Âº batch de U\n",
    "#         try:\n",
    "#             first_u = next(iter(U_ds))\n",
    "#             xw, xs, _ = first_u\n",
    "#             pw = tf.nn.softmax(model(xw, training=False) / temp_T, axis=-1)\n",
    "#             conf = tf.reduce_max(pw, axis=-1).numpy()\n",
    "#             conf_mean = float(conf.mean())\n",
    "#             conf_p90  = float(np.percentile(conf, 90))\n",
    "#         except StopIteration:\n",
    "#             conf_mean, conf_p90 = float('nan'), float('nan')\n",
    "\n",
    "#         print(f\"[epoch {epoch_idx}:{ep+1}/{epochs}]  tau={current_tau:.2f}  Î»_u={lam:.2f}  T={temp_T:.2f}  confÎ¼={conf_mean:.3f}  confP90={conf_p90:.3f}\")\n",
    "\n",
    "#         # ---------- supervisionado ----------\n",
    "#         tL = time.time()\n",
    "#         L_batches, L_loss_sum, L_acc_sum = 0, 0.0, 0.0\n",
    "#         for batch in L_ds:\n",
    "#             loss_sup, acc = train_step_supervised(batch)\n",
    "#             L_loss_sum += float(loss_sup)\n",
    "#             L_acc_sum  += float(acc)\n",
    "#             L_batches += 1\n",
    "#         if L_batches:\n",
    "#             print(f\"  L | batches={L_batches}  loss~={L_loss_sum/L_batches:.4f}  acc~={L_acc_sum/L_batches:.3f}  ({time.time()-tL:.1f}s)\")\n",
    "\n",
    "#         # ---------- nÃ£o-rotulado ----------\n",
    "#         tU = time.time()\n",
    "#         U_batches = 0\n",
    "#         mask_pass_sum = 0\n",
    "#         mask_total_sum = 0\n",
    "#         skipped_batches = 0\n",
    "#         U_u_sum = 0.0\n",
    "#         U_i_sum = 0.0\n",
    "\n",
    "#         for batch in U_ds:\n",
    "#             out = train_step_unlabeled_with_masklog(batch, lambda_u=lam, tau=current_tau, T=temp_T)\n",
    "\n",
    "#             U_batches += 1\n",
    "#             mask_pass_sum  += int(out[\"mask_pass\"].numpy())\n",
    "#             mask_total_sum += int(out[\"mask_total\"].numpy())\n",
    "#             skipped_batches += int(bool(out[\"skipped\"]))\n",
    "#             U_u_sum += float(tf.reduce_mean(out[\"u_batch\"]).numpy())\n",
    "#             U_i_sum += float(tf.reduce_mean(out[\"i_batch\"]).numpy())\n",
    "\n",
    "#         if mask_total_sum > 0:\n",
    "#             mask_rate = mask_pass_sum / mask_total_sum * 100.0\n",
    "#         else:\n",
    "#             mask_rate = 0.0\n",
    "\n",
    "#         print(f\"  U | batches={U_batches}  mask_rate={mask_rate:.1f}%  skipped={skipped_batches}  U~={U_u_sum/max(U_batches,1):.4f}  I~={U_i_sum/max(U_batches,1):.4f}  ({time.time()-tU:.1f}s)\")\n",
    "\n",
    "# def run_one_round(L_ds, U_ds, epoch_idx, epochs=5, \n",
    "#                   lambda_u_max=1.0, \n",
    "#                   base_tau=0.60, \n",
    "#                   temp_T=0.5,\n",
    "#                   target_rate_mild=0.60,   # ~60% passam na fase mild\n",
    "#                   target_rate_strong=0.50, # ~50% na fase forte\n",
    "#                   is_strong_phase=False,    # defina True/False fora conforme sua agenda mildâ†’strong\n",
    "#                   prev_tau=None):           # opcional: passa o tau da Ã©poca anterior p/ suavizar\n",
    "#     \"\"\"\n",
    "#     Retorna o Ãºltimo tau usado (para vocÃª reutilizar/suavizar na prÃ³xima chamada).\n",
    "#     \"\"\"\n",
    "#     tau_used = prev_tau if prev_tau is not None else base_tau\n",
    "\n",
    "#     for ep in range(epochs):\n",
    "#         # ---- 1) Supervisionado (igual)\n",
    "#         tL0 = time.time()\n",
    "#         l_loss_sum, l_acc_sum, l_batches = 0.0, 0.0, 0\n",
    "#         for batch in L_ds:\n",
    "#             loss_sup, acc = train_step_supervised(batch)\n",
    "#             l_loss_sum += float(loss_sup); l_acc_sum += float(acc); l_batches += 1\n",
    "#         tL1 = time.time()\n",
    "\n",
    "#         # ---- 2) Probe confianÃ§as da WEAK para escolher tau adaptativo desta Ã©poca\n",
    "#         confs = probe_confidences_on_U(model, U_ds, temp_T=temp_T,\n",
    "#                                        max_batches=32, stop_if_n=8192)\n",
    "#         target_rate = (target_rate_strong if is_strong_phase else target_rate_mild)\n",
    "#         tau_used = choose_adaptive_tau(confs, target_rate=target_rate, \n",
    "#                                        tau_min=0.40, tau_max=0.95,\n",
    "#                                        prev_tau=tau_used, smooth=0.6)\n",
    "\n",
    "#         # ---- 3) NÃ£o-rotulado (FixMatch) com tau adaptativo\n",
    "#         lam = lambda_u_warmup(ep, total=3, max_val=lambda_u_max)\n",
    "#         tU0 = time.time()\n",
    "#         u_batches, skipped_all = 0, 0\n",
    "#         pass_sum, total_sum = 0, 0\n",
    "#         u_mean_local, i_mean_local = 0.0, 0.0\n",
    "\n",
    "#         for batch in U_ds:\n",
    "#             out = train_step_unlabeled_with_masklog(\n",
    "#                 batch, lambda_u=lam, tau=tau_used, temp_T=temp_T\n",
    "#             )\n",
    "#             u_batches += 1\n",
    "#             skipped_all += int(out[\"skipped\"])\n",
    "\n",
    "#             # mÃ©tricas\n",
    "#             pass_sum  += int(out[\"mask_pass\"].numpy())\n",
    "#             total_sum += int(out[\"mask_total\"].numpy())\n",
    "#             u_mean_local += float(tf.reduce_mean(out[\"u_batch\"]))\n",
    "#             i_mean_local += float(tf.reduce_mean(out[\"i_batch\"]))\n",
    "\n",
    "#             # EMA/UCB\n",
    "#             u_vals  = out[\"u_batch\"].numpy()\n",
    "#             i_vals  = out[\"i_batch\"].numpy()\n",
    "#             sid_vals= out[\"sids\"].numpy()\n",
    "#             for val_u, val_i, sid in zip(u_vals, i_vals, sid_vals):\n",
    "#                 ema_update(U_MEAN, U_VAR, int(sid), float(val_u), alpha=0.9)\n",
    "#                 ema_update(I_MEAN, I_VAR, int(sid), float(val_i), alpha=0.9)\n",
    "\n",
    "#         tU1 = time.time()\n",
    "\n",
    "#         mask_rate = (pass_sum / max(total_sum, 1)) * 100.0\n",
    "#         print(f\"[epoch {epoch_idx}:{ep+1}/{epochs}]  \"\n",
    "#               f\"tau={tau_used:.2f}  Î»_u={lam:.2f}  T={temp_T:.2f}  \"\n",
    "#               f\"mask_rate={mask_rate:.1f}%  skipped_batches={skipped_all}/{u_batches}  \"\n",
    "#               f\"L_loss~={l_loss_sum/max(l_batches,1):.4f}  L_acc~={l_acc_sum/max(l_batches,1):.3f}  \"\n",
    "#               f\"U~={u_mean_local/max(u_batches,1):.4f}  I~={i_mean_local/max(u_batches,1):.4f}  \"\n",
    "#               f\"({tL1-tL0:.1f}s L, {tU1-tU0:.1f}s U)\")\n",
    "\n",
    "#     return tau_used\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82b14a2",
   "metadata": {},
   "source": [
    "### SeleÃ§Ã£o, simulaÃ§Ã£o de anotaÃ§Ã£o e loop ASSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fb5e0e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Pasta raiz jÃ¡ estÃ¡ no PYTHONPATH: C:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\ActiveSemisupervisedLearningCNN-s\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve() # Sobe um nÃ­vel e resolve o caminho absoluto\n",
    "\n",
    "# 2. Adiciona o caminho da raiz ao sys.path\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "    print(f\"âœ… Adicionado ao PYTHONPATH: {PROJECT_ROOT}\")\n",
    "else:\n",
    "    print(f\"ðŸ“ Pasta raiz jÃ¡ estÃ¡ no PYTHONPATH: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dbff35b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized EMA arrays length=22502\n",
      "L min/max: 0.0 1.0\n",
      "L min/max: 0.0 1.0\n",
      "U weak min/max: 0.0 1.0\n",
      "U strong min/max: 0.0 1.0\n",
      "== Rodada 1/10 ==\n",
      "U weak min/max: 0.0 1.0\n",
      "U strong min/max: 0.0 1.0\n",
      "== Rodada 1/10 ==\n",
      "train_step_supervised: elapsed 59.5304 s\n",
      "train_step_supervised: elapsed 59.5304 s\n",
      "train_step_supervised: elapsed 57.6342 s\n",
      "train_step_supervised: elapsed 57.6342 s\n",
      "train_step_supervised: elapsed 57.0532 s\n",
      "train_step_supervised: elapsed 57.0532 s\n",
      "train_step_supervised: elapsed 65.3285 s\n",
      "train_step_supervised: elapsed 65.3285 s\n",
      "train_step_supervised: elapsed 60.1511 s\n",
      "train_step_supervised: elapsed 60.1511 s\n",
      "train_step_supervised: elapsed 54.5186 s\n",
      "train_step_supervised: elapsed 54.5186 s\n",
      "train_step_supervised: elapsed 54.5945 s\n",
      "train_step_supervised: elapsed 54.5945 s\n",
      "train_step_supervised: elapsed 56.3752 s\n",
      "train_step_supervised: elapsed 56.3752 s\n",
      "train_step_supervised: elapsed 53.0958 s\n",
      "train_step_supervised: elapsed 53.0958 s\n",
      "train_step_supervised: elapsed 42.8052 s\n",
      "train_step_supervised: elapsed 42.8052 s\n",
      "train_step_supervised: elapsed 42.2889 s\n",
      "train_step_supervised: elapsed 42.2889 s\n",
      "train_step_supervised: elapsed 44.6257 s\n",
      "train_step_supervised: elapsed 44.6257 s\n",
      "train_step_supervised: elapsed 42.4861 s\n",
      "train_step_supervised: elapsed 42.4861 s\n",
      "train_step_supervised: elapsed 42.3429 s\n",
      "train_step_supervised: elapsed 42.3429 s\n",
      "train_step_supervised: elapsed 41.6255 s\n",
      "train_step_supervised: elapsed 41.6255 s\n",
      "train_step_supervised: elapsed 44.6408 s\n",
      "train_step_supervised: elapsed 44.6408 s\n",
      "train_step_supervised: elapsed 41.4421 s\n",
      "train_step_supervised: elapsed 41.4421 s\n",
      "train_step_supervised: elapsed 42.5132 s\n",
      "train_step_supervised: elapsed 42.5132 s\n",
      "train_step_supervised: elapsed 42.5816 s\n",
      "train_step_supervised: elapsed 42.5816 s\n",
      "train_step_supervised: elapsed 30.0801 s\n",
      "train_step_supervised: elapsed 30.0801 s\n",
      "train_step_supervised: elapsed 26.4269 s\n",
      "train_step_supervised: elapsed 26.4269 s\n",
      "train_step_supervised: elapsed 27.1713 s\n",
      "train_step_supervised: elapsed 27.1713 s\n",
      "train_step_supervised: elapsed 27.1461 s\n",
      "train_step_supervised: elapsed 27.1461 s\n",
      "train_step_supervised: elapsed 27.3688 s\n",
      "train_step_supervised: elapsed 27.3688 s\n",
      "train_step_supervised: elapsed 26.1602 s\n",
      "train_step_supervised: elapsed 26.1602 s\n",
      "unlabeled: forward weak took 0.3656 s\n",
      "unlabeled: forward weak took 0.3656 s\n",
      "unlabeled: forward strong(for I) took 0.3790 s\n",
      "unlabeled: forward strong(for I) took 0.3790 s\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute Sub as input #1(zero-based) was expected to be a half tensor but is a float tensor [Op:Sub] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 146\u001b[39m\n\u001b[32m    142\u001b[39m strong_aug = make_strong_aug(mild=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    143\u001b[39m U_ds = make_U_ds(U_records, batch=BATCH, image_size=IMAGE_SIZE,\n\u001b[32m    144\u001b[39m                  weak_aug=weak_aug, strong_aug=strong_aug, cache=CACHE_DATA)\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m \u001b[43mrun_one_round\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mU_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_u_max\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_T\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# --- depois, forÃ§a total ---\u001b[39;00m\n\u001b[32m    149\u001b[39m strong_aug = make_strong_aug(mild=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mrun_one_round\u001b[39m\u001b[34m(L_ds, U_ds, epoch_idx, epochs, lambda_u_max, base_tau, temp_T, target_rate_mild, target_rate_strong, is_strong_phase, prev_tau)\u001b[39m\n\u001b[32m     36\u001b[39m u_mean_local, i_mean_local = \u001b[32m0.0\u001b[39m, \u001b[32m0.0\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m U_ds:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     out = \u001b[43mtrain_step_unlabeled_with_masklog\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_u\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtau_used\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_T\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemp_T\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m     u_batches += \u001b[32m1\u001b[39m\n\u001b[32m     43\u001b[39m     skipped_all += \u001b[38;5;28mint\u001b[39m(out[\u001b[33m\"\u001b[39m\u001b[33mskipped\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mtrain_step_unlabeled_with_masklog\u001b[39m\u001b[34m(batch, lambda_u, tau, temp_T)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DEBUG \u001b[38;5;129;01mand\u001b[39;00m t1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     52\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33munlabeled: forward strong(for I) took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()-t1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m s\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m u_batch = \u001b[43mpseudo_el2n\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs_w\u001b[49m\u001b[43m)\u001b[49m                   \u001b[38;5;66;03m# [B]\u001b[39;00m\n\u001b[32m     55\u001b[39m i_batch = sym_kl(probs_w, probs_s_for_I)         \u001b[38;5;66;03m# [B]\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Sem exemplos vÃ¡lidos ou sem peso â†’ nÃ£o atualiza\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mpseudo_el2n\u001b[39m\u001b[34m(probs)\u001b[39m\n\u001b[32m      6\u001b[39m yhat = tf.argmax(probs, axis=-\u001b[32m1\u001b[39m)\n\u001b[32m      7\u001b[39m oh = tf.one_hot(yhat, depth=tf.shape(probs)[-\u001b[32m1\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tf.reduce_mean(tf.square(\u001b[43mprobs\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43moh\u001b[49m), axis=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\ActiveSemisupervisedLearningCNN-s\\tf_env\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    155\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\ActiveSemisupervisedLearningCNN-s\\tf_env\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6027\u001b[39m, in \u001b[36mraise_from_not_ok_status\u001b[39m\u001b[34m(e, name)\u001b[39m\n\u001b[32m   6025\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraise_from_not_ok_status\u001b[39m(e, name) -> NoReturn:\n\u001b[32m   6026\u001b[39m   e.message += (\u001b[33m\"\u001b[39m\u001b[33m name: \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m-> \u001b[39m\u001b[32m6027\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m core._status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mInvalidArgumentError\u001b[39m: cannot compute Sub as input #1(zero-based) was expected to be a half tensor but is a float tensor [Op:Sub] name: "
     ]
    }
   ],
   "source": [
    "def acquire_topK_balanced(U_records, K, class_names):\n",
    "    import time\n",
    "    start = time.time()\n",
    "\n",
    "    per_class = max(1, K // max(1, len(class_names)))\n",
    "    buckets = {name: [] for name in class_names}\n",
    "\n",
    "    for rec in U_records:\n",
    "        sid = rec[\"sid\"]\n",
    "        p = rec[\"path\"].replace(\"\\\\\",\"/\").lower()\n",
    "        cname = None\n",
    "        for name in class_names:\n",
    "            if f\"/{name.lower()}/\" in p:\n",
    "                cname = name; break\n",
    "        if cname is None:\n",
    "            continue\n",
    "        # prefer array stats when available for speed\n",
    "        # if U_MEAN_ARR is not None:\n",
    "        #     mu_u, vu_u = float(U_MEAN_ARR[sid]), float(U_VAR_ARR[sid])\n",
    "        # else:\n",
    "        mu_u, vu_u = U_MEAN.get(sid, 0.0), U_VAR.get(sid, 0.0)\n",
    "        # if I_MEAN_ARR is not None:\n",
    "        #     mu_i, vi_i = float(I_MEAN_ARR[sid]), float(I_VAR_ARR[sid])\n",
    "        # else:\n",
    "        mu_i, vi_i = I_MEAN.get(sid, 0.0), I_VAR.get(sid, 0.0)\n",
    "        score = ucb(mu_u, vu_u, c_u) * ucb(mu_i, vi_i, c_i)\n",
    "        buckets[cname].append((score, sid))\n",
    "\n",
    "    selected = []\n",
    "    for cname in class_names:\n",
    "        cand = sorted(buckets[cname], key=lambda x: x[0], reverse=True)\n",
    "        selected.extend([sid for _, sid in cand[:per_class]])\n",
    "\n",
    "    if len(selected) < K:\n",
    "        rest = []\n",
    "        have = set(selected)\n",
    "        for rec in U_records:\n",
    "            sid = rec[\"sid\"]\n",
    "            if sid in have: \n",
    "                continue\n",
    "            if U_MEAN_ARR is not None:\n",
    "                mu_u, vu_u = float(U_MEAN_ARR[sid]), float(U_VAR_ARR[sid])\n",
    "            else:\n",
    "                mu_u, vu_u = U_MEAN.get(sid, 0.0), U_VAR.get(sid, 0.0)\n",
    "            if I_MEAN_ARR is not None:\n",
    "                mu_i, vi_i = float(I_MEAN_ARR[sid]), float(I_VAR_ARR[sid])\n",
    "            else:\n",
    "                mu_i, vi_i = I_MEAN.get(sid, 0.0), I_VAR.get(sid, 0.0)\n",
    "            rest.append((ucb(mu_u,vu_u,c_u)*ucb(mu_i,vi_i,c_i), sid))\n",
    "        rest.sort(key=lambda x: x[0], reverse=True)\n",
    "        selected.extend([sid for _, sid in rest[:K-len(selected)]])\n",
    "\n",
    "    print(f\"acquire_topK_balanced: elapsed {time.time()-start:.4f} s\")\n",
    "    return selected[:K]\n",
    "\n",
    "def simulate_sid2true_from_path(U_records, L_records, class_map=CLASS_MAP):\n",
    "    sid2true = {}\n",
    "    def infer(path):\n",
    "        p = path.replace(\"\\\\\",\"/\").lower()\n",
    "        for name, idx in class_map.items():\n",
    "            if f\"/{name.lower()}/\" in p:\n",
    "                return idx\n",
    "        raise ValueError(f\"NÃ£o infere classe de: {path}\")\n",
    "    for rec in U_records: sid2true[rec[\"sid\"]] = infer(rec[\"path\"])\n",
    "    for rec in L_records: sid2true[rec[\"sid\"]] = infer(rec[\"path\"])\n",
    "    return sid2true\n",
    "\n",
    "\n",
    "def move_annotated_to_L(sid2label, U_records, L_records):\n",
    "    by_id = {r[\"sid\"]: r for r in U_records}\n",
    "    moved = []\n",
    "    keepU = []\n",
    "    for r in U_records:\n",
    "        sid = r[\"sid\"]\n",
    "        if sid in sid2label:\n",
    "            nr = dict(r)\n",
    "            nr[\"label\"] = int(sid2label[sid])\n",
    "            moved.append(nr)\n",
    "        else:\n",
    "            keepU.append(r)\n",
    "    return keepU, (L_records + moved)\n",
    "\n",
    "# ==== Dataset base ====\n",
    "train_root = r\"..\\data\\train\"\n",
    "val_root   = r\"..\\data\\validation\"\n",
    "\n",
    "all_train = build_records_from_dir(train_root, class_map=CLASS_MAP)\n",
    "L_records, U_records = split_L_U(all_train, n_L=400, seed=42)\n",
    "\n",
    "# initialize EMA arrays if possible (vectorized updates)\n",
    "try:\n",
    "    max_sid = max(r['sid'] for r in all_train)\n",
    "    initialize_ema_arrays(max_sid+1)\n",
    "except Exception as _e:\n",
    "    print(\"Could not initialize EMA arrays:\", _e)\n",
    "\n",
    "# Decide caching heuristically (if psutil available)\n",
    "# if CACHE_DATA is None:\n",
    "#     try:\n",
    "#         import psutil\n",
    "#         avail = psutil.virtual_memory().available\n",
    "#         # estimated RAM (float32) for all images in memory: 4 bytes * H * W * 3 channels * N\n",
    "#         N = len(all_train)\n",
    "#         est_bytes = 4 * H * W * 3 * N\n",
    "#         # require <= 40% of available memory\n",
    "#         CACHE_DATA = (est_bytes < (0.4 * avail))\n",
    "#         print(f\"Caching heuristic: est_bytes={est_bytes:,} avail={avail:,} -> CACHE_DATA={CACHE_DATA}\")\n",
    "#     except Exception as _e:\n",
    "#         CACHE_DATA = False\n",
    "#         print(\"psutil not available; defaulting CACHE_DATA=False\")\n",
    "CACHE_DATA=True\n",
    "\n",
    "val_records = build_records_from_dir(val_root, class_map=CLASS_MAP)\n",
    "val_ds = make_val_ds(val_records, batch=BATCH, image_size=IMAGE_SIZE)\n",
    "\n",
    "CLASS_NAMES = list(CLASS_MAP.keys())  # ['Cat','Dog'] etc.\n",
    "weak_aug   = make_weak_aug()\n",
    "strong_aug = make_strong_aug(mild=True)\n",
    "L_ds = make_L_ds(L_records, batch=BATCH, image_size=IMAGE_SIZE, cache=CACHE_DATA)\n",
    "U_ds = make_U_ds(U_records, batch=BATCH, image_size=IMAGE_SIZE, cache=CACHE_DATA,\n",
    "                 weak_aug=weak_aug, strong_aug=strong_aug)\n",
    "\n",
    "# Checar L\n",
    "xb, yb, _ = next(iter(L_ds))\n",
    "print(\"L min/max:\", float(tf.reduce_min(xb)), float(tf.reduce_max(xb)))\n",
    "\n",
    "# Checar U (weak/strong)\n",
    "xw, xs, _ = next(iter(U_ds))\n",
    "print(\"U weak min/max:\", float(tf.reduce_min(xw)), float(tf.reduce_max(xw)))\n",
    "print(\"U strong min/max:\", float(tf.reduce_min(xs)), float(tf.reduce_max(xs)))\n",
    "\n",
    "# simulaÃ§Ã£o de anotaÃ§Ã£o (sem arquivos)\n",
    "sid2true = simulate_sid2true_from_path(U_records, L_records, class_map=CLASS_MAP)\n",
    "\n",
    "ROUNDS = 10\n",
    "EPOCHS_PER_ROUND = 5\n",
    "\n",
    "for r in range(ROUNDS):\n",
    "    print(f\"== Rodada {r+1}/{ROUNDS} ==\")\n",
    "\n",
    "    # --- primeiras Ã©pocas da rodada: mild ---\n",
    "    strong_aug = make_strong_aug(mild=True)\n",
    "    U_ds = make_U_ds(U_records, batch=BATCH, image_size=IMAGE_SIZE,\n",
    "                     weak_aug=weak_aug, strong_aug=strong_aug, cache=CACHE_DATA)\n",
    "\n",
    "    run_one_round(L_ds, U_ds, epoch_idx=r, epochs=2, lambda_u_max=1.0, temp_T=0.5)\n",
    "\n",
    "    # --- depois, forÃ§a total ---\n",
    "    strong_aug = make_strong_aug(mild=False)\n",
    "    U_ds = make_U_ds(U_records, batch=BATCH, image_size=IMAGE_SIZE,\n",
    "                     weak_aug=weak_aug, strong_aug=strong_aug, cache=CACHE_DATA)\n",
    "\n",
    "    run_one_round(L_ds, U_ds, epoch_idx=r, epochs=EPOCHS_PER_ROUND-2, lambda_u_max=1.0, temp_T=0.5)\n",
    "\n",
    "    # avaliaÃ§Ã£o\n",
    "    vloss, vacc = evaluate(model, val_ds)\n",
    "    print(f\"Val loss: {vloss:.4f}  Val acc: {vacc:.4f}\")\n",
    "\n",
    "    # seleÃ§Ã£o ativa balanceada\n",
    "    selected_ids = acquire_topK_balanced(U_records, K=40, class_names=CLASS_NAMES) # verificar qual a melhor qtd de amostra ideal\n",
    "\n",
    "    # â€œanotaÃ§Ã£oâ€ simulada apenas nos K escolhidos\n",
    "    sid2label = {sid: sid2true[sid] for sid in selected_ids}\n",
    "    # mover U->L\n",
    "    U_records, L_records = move_annotated_to_L(sid2label, U_records, L_records)\n",
    "\n",
    "    # limpar EMA/UCB dos sids movidos\n",
    "    moved_set = set(sid2label.keys())\n",
    "    # if U_MEAN_ARR is not None:\n",
    "    #     # zero out arrays for moved sids (cheap)\n",
    "    #     for sid in moved_set:\n",
    "    #         U_MEAN_ARR[sid] = 0.0; U_VAR_ARR[sid] = 0.0\n",
    "    #         I_MEAN_ARR[sid] = 0.0; I_VAR_ARR[sid] = 0.0\n",
    "    # else:\n",
    "    for sid in list(U_MEAN.keys()):\n",
    "        if sid in moved_set: U_MEAN.pop(sid, None); U_VAR.pop(sid, None)\n",
    "    for sid in list(I_MEAN.keys()):\n",
    "        if sid in moved_set: I_MEAN.pop(sid, None); I_VAR.pop(sid, None)\n",
    "\n",
    "    # rebuild datasets\n",
    "    L_ds = make_L_ds(L_records, batch=BATCH, image_size=IMAGE_SIZE, cache=CACHE_DATA)\n",
    "    U_ds = make_U_ds(U_records, batch=BATCH, image_size=IMAGE_SIZE, cache=CACHE_DATA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6272703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard profiler quick-run snippet (executa uma pequena carga e escreve perfil em logs/profile/)\n",
    "import datetime\n",
    "logdir = \"logs/profile/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "print(\"Profiling to:\", logdir)\n",
    "try:\n",
    "    tf.profiler.experimental.start(logdir)\n",
    "    # execute uma rodada curta (1 rodada, poucas Ã©pocas) ou um pequeno trecho do pipeline\n",
    "    run_one_round(L_ds, U_ds, epoch_idx=0, epochs=1, lambda_u_max=0.1, temp_T=0.5)\n",
    "    tf.profiler.experimental.stop()\n",
    "    print(\"Profiler finished. Use: tensorboard --logdir=logs/profile --port=6006\")\n",
    "except Exception as e:\n",
    "    print(\"Profiler run failed:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
